import json
import os
import re
import requests
import hashlib
from datetime import datetime

# ================= é…ç½®åŒºåŸŸ =================
API_KEY = "sk-7ba052d40efe48ae990141e577d952d1"  # 
API_URL = "https://api.deepseek.com/chat/completions"
MODEL_NAME = "deepseek-chat"  # 

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_DIR = os.path.join(BASE_DIR, 'database', 'raw')
OUTPUT_DIR = os.path.join(BASE_DIR, 'public', 'db', 'account')

FILENAME_MAPPING = {
    "Taiwan": "Taiwan",
    "US": "US",
    "Philippines": "Philippines",
    "Japan": "Japan",
    "JP": "Japan",
    "ph": "Philippines",
    "us": "US",
    "jp": "Japan",
    "tw": "Taiwan"
}
# ===========================================

def parse_date_from_filename(filename):
    match = re.search(r'(\d{8})_(\d{6})', filename)
    if match:
        date_str = match.group(1)
        return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
    return None

def get_files_fingerprint(date_key):
    target_date_str = date_key.replace("-", "")
    related_files = []
    if os.path.exists(RAW_DIR):
        for root, dirs, files in os.walk(RAW_DIR):
            for f in files:
                if target_date_str in f and f.endswith('.json'):
                    path = os.path.join(root, f)
                    stat = os.stat(path)
                    related_files.append(f"{f}_{stat.st_size}_{stat.st_mtime}")
    if not related_files: return None
    related_files.sort()
    combined_str = "|".join(related_files)
    return hashlib.md5(combined_str.encode('utf-8')).hexdigest()

def check_needs_update(output_file, current_fingerprint):
    if not os.path.exists(output_file): return True
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            saved_fingerprint = data.get('_meta', {}).get('fingerprint', '')
            return saved_fingerprint != current_fingerprint
    except: return True

def load_and_group_by_date_user():
    grouped_data = {}
    if not os.path.exists(RAW_DIR): return grouped_data

    print(f"ğŸ“‚ æ­£åœ¨æ‰«æ {RAW_DIR} ...")
    for root, dirs, files in os.walk(RAW_DIR):
        for filename in files:
            if not filename.endswith('.json'): continue
            date_key = parse_date_from_filename(filename)
            if not date_key: continue
            
            target_region = None
            for key, region in FILENAME_MAPPING.items():
                if key.lower() in filename.lower():
                    target_region = region
                    break
            if not target_region: continue

            if date_key not in grouped_data: grouped_data[date_key] = {}
            if target_region not in grouped_data[date_key]: grouped_data[date_key][target_region] = {}

            path = os.path.join(root, filename)
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    items = data if isinstance(data, list) else [data]
                    
                    for item in items:
                        uname = item.get('username', 'Unknown')
                        # å­˜å‚¨å®Œæ•´å¯¹è±¡ä»¥ä¾¿åç»­ä½¿ç”¨
                        if item.get('full_text'):
                            if uname not in grouped_data[date_key][target_region]:
                                grouped_data[date_key][target_region][uname] = []
                            grouped_data[date_key][target_region][uname].append(item)
            except: pass

    return grouped_data

def analyze_user_profile(username, raw_tweets):
    """
    è°ƒç”¨ LLM åˆ†æç”¨æˆ·ç”»åƒ + æ¨æ–‡ç«‹åœº
    """
    if not raw_tweets: return None

    # 1. æ™ºèƒ½é‡‡æ ·ï¼šæŒ‰äº’åŠ¨é‡æ’åºï¼Œå– Top 15
    def calculate_impact(item):
        return (item.get('retweet_count', 0)*2) + item.get('reply_count', 0) + (item.get('favorite_count', 0)*0.5)
    
    # å¤åˆ¶ä¸€ä»½å¹¶æ’åºï¼Œä»¥å…å½±å“åŸåˆ—è¡¨
    sorted_tweets = sorted(raw_tweets, key=calculate_impact, reverse=True)
    # é™åˆ¶åˆ†æ Top 15 æ¡ï¼Œé˜²æ­¢ Token çˆ†ç‚¸
    top_tweets = sorted_tweets[:15]
    
    # 2. æ„å»ºå¸¦ ID çš„è¾“å…¥
    input_list = []
    for idx, t in enumerate(top_tweets):
        text = t.get('full_text', '').replace('\n', ' ').strip()
        if len(text) > 10:
            input_list.append(f"ID[{idx}]: {text}")
    
    input_text_str = "\n".join(input_list)
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªç¤¾ä¼šå¿ƒç†å­¦ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ· "{username}" çš„æ¨æ–‡è®°å½•è¿›è¡Œåˆ†æã€‚
    
    æ¨æ–‡åˆ—è¡¨ (å¸¦ID):
    {input_text_str}

    ä»»åŠ¡ï¼š
    1. ã€ç”»åƒç”Ÿæˆã€‘
       - info: ä¸€å¥è¯æ¦‚æ‹¬äººè®¾(50å­—å†…)ã€‚
       - stance_matrix: å¯¹ä¸­ç«‹åœºçŸ©é˜µ [[x(ç«‹åœº0-2), y(ç»´åº¦0-3), value(0-10)]...]ã€‚ç»´åº¦:0æ”¿1å†›2ç»3æ–‡; ç«‹åœº:0è´Ÿ1ä¸­2æ­£ã€‚
       - influence_type: äº²æƒ…/åŒä¼´/æƒå¨ ä¸‰ç±»å æ¯”ã€‚
    
    2. ã€æ¨æ–‡ç ”åˆ¤ã€‘
       - é’ˆå¯¹æä¾›çš„æ¯ä¸€æ¡æ¨æ–‡IDï¼Œåˆ¤æ–­å…¶å…·ä½“çš„ç«‹åœº (positive/neutral/negative)ã€‚
    
    è¾“å‡º JSON æ ¼å¼ï¼š
    {{
        "info": "...",
        "stance_matrix": [[0,0,8]...], 
        "influence_type": [{{"name": "æƒå¨ (Authority)", "value": 80}}...],
        "tweet_analysis": [
            {{"id": 0, "stance": "negative"}},
            {{"id": 1, "stance": "neutral"}}
        ]
    }}
    """

    try:
        response = requests.post(API_URL, json={
            "model": MODEL_NAME,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "response_format": {"type": "json_object"}
        }, headers={"Authorization": f"Bearer {API_KEY}"})
        
        if response.status_code == 200:
            res_json = json.loads(response.json()['choices'][0]['message']['content'])
            
            # 3. æ•°æ®å›å¡«ï¼šæŠŠ LLM çš„ç«‹åœºåˆ¤æ–­ä¸åŸå§‹æ¨æ–‡æ‹¼åˆ
            enriched_tweets = []
            analysis_map = {item['id']: item['stance'] for item in res_json.get('tweet_analysis', [])}
            
            for idx, tweet in enumerate(top_tweets):
                stance = analysis_map.get(idx, 'neutral') # é»˜è®¤ä¸­æ€§
                enriched_tweets.append({
                    "text": tweet.get('full_text', ''),
                    "stance": stance,
                    "username": tweet.get('username', username),
                    "created_at": tweet.get('created_at', ''),
                    "metrics": {
                        "reply": tweet.get('reply_count', 0),
                        "retweet": tweet.get('retweet_count', 0),
                        "like": tweet.get('favorite_count', 0)
                    }
                })
            
            # è¿”å›ç»„åˆå¥½çš„æ•°æ®
            return {
                "info": res_json.get("info"),
                "stance_matrix": res_json.get("stance_matrix"),
                "influence_type": res_json.get("influence_type"),
                "tweets": enriched_tweets
            }

    except Exception as e:
        print(f"Error analyzing {username}: {e}")
    return None

def main():
    print("ğŸš€ å¼€å§‹æŒ‰æ—¥æœŸå¤„ç†è´¦å·æ¨èæ•°æ® (æ¨æ–‡æº¯æºç‰ˆ)...")
    date_groups = load_and_group_by_date_user()
    
    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)

    for date_key, regions_data in date_groups.items():
        out_path = os.path.join(OUTPUT_DIR, f"{date_key}.json")
        current_fingerprint = get_files_fingerprint(date_key)
        
        if not check_needs_update(out_path, current_fingerprint):
            print(f"â© æ—¥æœŸ {date_key} æœªå˜åŠ¨ï¼Œè·³è¿‡")
            continue

        print(f"\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"ğŸ”„ æ­£åœ¨åˆ†ææ—¥æœŸ: {date_key}")
        
        daily_result = {} 
        
        for region, users_map in regions_data.items():
            print(f"   -> æ¿å— [{region}] å…±æœ‰ {len(users_map)} ä¸ªæ´»è·ƒç”¨æˆ·")
            
            # ç®€å•æ’åºï¼šå–å‘å¸–é‡æœ€å¤šçš„ Top 5
            sorted_users = sorted(users_map.items(), key=lambda x: len(x[1]), reverse=True)[:5]
            
            analyzed_list = []
            for uname, tweets in sorted_users:
                print(f"      æ­£åœ¨ç”»åƒ: {uname} (åŸºäº {min(len(tweets), 15)} æ¡é«˜çƒ­åº¦æ¨æ–‡)...")
                
                profile = analyze_user_profile(uname, tweets)
                
                if profile:
                    profile['username'] = uname
                    profile['tweet_count'] = len(tweets)
                    analyzed_list.append(profile)
            
            daily_result[region] = {
                "region": region,
                "time_range": [date_key, date_key],
                "top_users": analyzed_list
            }
        
        daily_result["_meta"] = {
            "fingerprint": current_fingerprint,
            "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
            
        with open(out_path, 'w', encoding='utf-8') as f:
            json.dump(daily_result, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ›´æ–°æˆåŠŸ: {out_path}")

    print("\nğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()