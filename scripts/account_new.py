import json
import os
import re
import requests
import hashlib
from datetime import datetime

# ================= é…ç½®åŒºåŸŸ =================
# ğŸ“… ã€æ ¸å¿ƒä¿®æ”¹ã€‘åœ¨è¿™é‡ŒæŒ‡å®šä½ è¦å¤„ç†çš„æ—¥æœŸ
TARGET_DATE = "2025-12-25"

API_KEY = "sk-7ba052d40efe48ae990141e577d952d1"  # 
API_URL = "https://api.deepseek.com/chat/completions"
MODEL_NAME = "deepseek-chat"  # 

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_DIR = os.path.join(BASE_DIR, 'database', 'raw')
OUTPUT_DIR = os.path.join(BASE_DIR, 'public', 'db', 'account')

FILENAME_MAPPING = {
    "Taiwan": "Taiwan",
    "US": "US",
    "Philippines": "Philippines",
    "Japan": "Japan",
    "JP": "Japan",
    "ph": "Philippines",
    "us": "US",
    "jp": "Japan",
    "tw": "Taiwan"
}
# ===========================================

def parse_date_from_filename(filename):
    """
    ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸ
    å…¼å®¹æ ¼å¼ï¼š
    1. ...20251225_211308.json (å¸¦æ—¶é—´)
    2. ...20251225.json        (ä¸å¸¦æ—¶é—´)
    """
    # ä¿®æ”¹æ­£åˆ™ï¼šåªæŠ“å–è¿ç»­çš„8ä½æ•°å­— (YYYYMMDD)
    # 20[2-3]\d è¡¨ç¤ºåŒ¹é… 2020-2039 å¹´ï¼Œé˜²æ­¢è¯¯åˆ¤å…¶ä»–æ•°å­—
    match = re.search(r'(20[2-3]\d{5})', filename)
    
    if match:
        date_str = match.group(1) # æ‹¿åˆ° 20251225
        # æ ¼å¼åŒ–ä¸º 2025-12-25
        return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
    return None

def get_files_fingerprint(date_key):
    """è®¡ç®—ç›®æ ‡æ—¥æœŸä¸‹ç›¸å…³æ–‡ä»¶çš„æŒ‡çº¹"""
    target_date_str = date_key.replace("-", "")
    related_files = []
    if os.path.exists(RAW_DIR):
        for root, dirs, files in os.walk(RAW_DIR):
            for f in files:
                if target_date_str in f and f.endswith('.json'):
                    path = os.path.join(root, f)
                    stat = os.stat(path)
                    related_files.append(f"{f}_{stat.st_size}_{stat.st_mtime}")
    if not related_files: return None
    related_files.sort()
    combined_str = "|".join(related_files)
    return hashlib.md5(combined_str.encode('utf-8')).hexdigest()

def check_needs_update(output_file, current_fingerprint):
    if not os.path.exists(output_file): return True
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            saved_fingerprint = data.get('_meta', {}).get('fingerprint', '')
            return saved_fingerprint != current_fingerprint
    except: return True

def load_data_for_target_date(target_date):
    """
    ã€ä¿®æ”¹ã€‘åªåŠ è½½æŒ‡å®šæ—¥æœŸçš„æ•°æ®
    è¿”å›ç»“æ„: { "US": { "user1": [tweets...], ... }, "Japan": ... }
    """
    grouped_data = {}
    
    # å°† 2025-12-25 è½¬æ¢ä¸º 20251225 ä»¥åŒ¹é…æ–‡ä»¶å
    target_date_str = target_date.replace("-", "")
    
    if not os.path.exists(RAW_DIR): return grouped_data

    print(f"ğŸ“‚ æ­£åœ¨æ‰«æ {RAW_DIR} ä¸­åŒ…å« '{target_date_str}' çš„æ–‡ä»¶...")
    file_count = 0
    
    for root, dirs, files in os.walk(RAW_DIR):
        for filename in files:
            if not filename.endswith('.json'): continue
            
            # 1. ä¸¥æ ¼åŒ¹é…æ—¥æœŸå­—ç¬¦ä¸²
            if target_date_str not in filename: continue
            
            # 2. è¯†åˆ«æ¿å—
            target_region = None
            for key, region in FILENAME_MAPPING.items():
                if key.lower() in filename.lower():
                    target_region = region
                    break
            if not target_region: continue

            # 3. åˆå§‹åŒ–
            if target_region not in grouped_data: grouped_data[target_region] = {}

            path = os.path.join(root, filename)
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    items = data if isinstance(data, list) else [data]
                    
                    for item in items:
                        uname = item.get('username', 'Unknown')
                        if item.get('full_text'):
                            if uname not in grouped_data[target_region]:
                                grouped_data[target_region][uname] = []
                            grouped_data[target_region][uname].append(item)
                    file_count += 1
            except: pass

    print(f"âœ… æ‰«æå®Œæˆï¼Œå…±æ‰¾åˆ° {file_count} ä¸ªç›¸å…³æ–‡ä»¶")
    return grouped_data

def analyze_user_profile(username, raw_tweets):
    """
    è°ƒç”¨ LLM åˆ†æç”¨æˆ·ç”»åƒ + æ¨æ–‡ç«‹åœº
    """
    if not raw_tweets: return None

    # 1. æ™ºèƒ½é‡‡æ ·ï¼šæŒ‰äº’åŠ¨é‡æ’åºï¼Œå– Top 15
    def calculate_impact(item):
        return (item.get('retweet_count', 0)*2) + item.get('reply_count', 0) + (item.get('favorite_count', 0)*0.5)
    
    # å¤åˆ¶ä¸€ä»½å¹¶æ’åº
    sorted_tweets = sorted(raw_tweets, key=calculate_impact, reverse=True)
    top_tweets = sorted_tweets[:15]
    
    # 2. æ„å»ºè¾“å…¥
    input_list = []
    for idx, t in enumerate(top_tweets):
        text = t.get('full_text', '').replace('\n', ' ').strip()
        if len(text) > 10:
            input_list.append(f"ID[{idx}]: {text}")
    
    input_text_str = "\n".join(input_list)
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªç¤¾ä¼šå¿ƒç†å­¦ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ· "{username}" çš„æ¨æ–‡è®°å½•è¿›è¡Œåˆ†æã€‚
    
    æ¨æ–‡åˆ—è¡¨ (å¸¦ID):
    {input_text_str}

    ä»»åŠ¡ï¼š
    1. ã€ç”»åƒç”Ÿæˆã€‘
       - info: ä¸€å¥è¯æ¦‚æ‹¬äººè®¾(50å­—å†…)ã€‚
       - stance_matrix: å¯¹ä¸­ç«‹åœºçŸ©é˜µ [[x(ç«‹åœº0-2), y(ç»´åº¦0-3), value(0-10)]...]ã€‚ç»´åº¦:0æ”¿1å†›2ç»3æ–‡; ç«‹åœº:0è´Ÿ1ä¸­2æ­£ã€‚
       - influence_type: äº²æƒ…/åŒä¼´/æƒå¨ ä¸‰ç±»å æ¯”ã€‚
    
    2. ã€æ¨æ–‡ç ”åˆ¤ã€‘
       - é’ˆå¯¹æä¾›çš„æ¯ä¸€æ¡æ¨æ–‡IDï¼Œåˆ¤æ–­å…¶å…·ä½“çš„ç«‹åœº (positive/neutral/negative)ã€‚
    
    è¾“å‡º JSON æ ¼å¼ï¼š
    {{
        "info": "...",
        "stance_matrix": [[0,0,8]...], 
        "influence_type": [{{"name": "æƒå¨ (Authority)", "value": 80}}...],
        "tweet_analysis": [
            {{"id": 0, "stance": "negative"}},
            {{"id": 1, "stance": "neutral"}}
        ]
    }}
    """

    try:
        response = requests.post(API_URL, json={
            "model": MODEL_NAME,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "response_format": {"type": "json_object"}
        }, headers={"Authorization": f"Bearer {API_KEY}"})
        
        if response.status_code == 200:
            res_json = json.loads(response.json()['choices'][0]['message']['content'])
            
            # 3. æ•°æ®å›å¡«
            enriched_tweets = []
            analysis_map = {item['id']: item['stance'] for item in res_json.get('tweet_analysis', [])}
            
            for idx, tweet in enumerate(top_tweets):
                stance = analysis_map.get(idx, 'neutral') 
                enriched_tweets.append({
                    "text": tweet.get('full_text', ''),
                    "stance": stance,
                    "username": tweet.get('username', username),
                    "created_at": tweet.get('created_at', ''),
                    "metrics": {
                        "reply": tweet.get('reply_count', 0),
                        "retweet": tweet.get('retweet_count', 0),
                        "like": tweet.get('favorite_count', 0)
                    }
                })
            
            return {
                "info": res_json.get("info"),
                "stance_matrix": res_json.get("stance_matrix"),
                "influence_type": res_json.get("influence_type"),
                "tweets": enriched_tweets
            }

    except Exception as e:
        print(f"Error analyzing {username}: {e}")
    return None

def main():
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œå•æ—¥è´¦å·ç”»åƒåˆ†æ | ç›®æ ‡æ—¥æœŸ: {TARGET_DATE}")
    
    # 1. æ£€æŸ¥æŒ‡çº¹
    out_path = os.path.join(OUTPUT_DIR, f"{TARGET_DATE}.json")
    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)
    
    # 2. åŠ è½½æ•°æ®
    regions_data = load_data_for_target_date(TARGET_DATE)
    
    if not regions_data:
        print(f"âš ï¸ æœªæ‰¾åˆ°æ—¥æœŸ {TARGET_DATE} çš„ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶åã€‚")
        return

    # 3. å¼€å§‹å¤„ç†
    print(f"\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"ğŸ”„ æ­£åœ¨åˆ†æ: {TARGET_DATE}")
    
    daily_result = {}
    current_fingerprint = get_files_fingerprint(TARGET_DATE)
    
    # æ™ºèƒ½æ›´æ–°æ£€æŸ¥ (æŒ‡çº¹å¯¹æ¯”)
    if not check_needs_update(out_path, current_fingerprint):
         print(f"â© æ—¥æœŸ {TARGET_DATE} æ•°æ®æœªå˜åŠ¨ï¼Œè·³è¿‡å¤„ç† (å·²èŠ‚çœ Token)")
         return

    for region, users_map in regions_data.items():
        print(f"   -> æ¿å— [{region}] å…±æœ‰ {len(users_map)} ä¸ªæ´»è·ƒç”¨æˆ·")
        
        # å–å‘å¸–é‡æœ€å¤šçš„ Top 5
        sorted_users = sorted(users_map.items(), key=lambda x: len(x[1]), reverse=True)[:5]
        
        analyzed_list = []
        for uname, tweets in sorted_users:
            print(f"      æ­£åœ¨ç”»åƒ: {uname} (åŸºäº {min(len(tweets), 15)} æ¡é«˜çƒ­åº¦æ¨æ–‡)...")
            profile = analyze_user_profile(uname, tweets)
            
            if profile:
                profile['username'] = uname
                profile['tweet_count'] = len(tweets)
                analyzed_list.append(profile)
        
        daily_result[region] = {
            "region": region,
            "time_range": [TARGET_DATE, TARGET_DATE],
            "top_users": analyzed_list
        }
    
    # 4. å†™å…¥æ–‡ä»¶
    daily_result["_meta"] = {
        "fingerprint": current_fingerprint,
        "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
        
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(daily_result, f, ensure_ascii=False, indent=2)
    print(f"âœ… ç”ŸæˆæˆåŠŸ: {out_path}")

if __name__ == "__main__":
    main()