import json
import os
import re
import requests
import hashlib
import sys
import time
import math
from datetime import datetime

# ================= é…ç½®åŒºåŸŸ =================
if len(sys.argv) > 1:
    TARGET_DATE = sys.argv[1]
else:
    TARGET_DATE = "2026-01-21"

# âš ï¸ 1. æ‰¹å¤„ç†å¤§å°ï¼šä¿æŒ 15-20
BATCH_SIZE = 20
# âš ï¸ 2. æœ€å¤§é™åˆ¶ï¼šæƒ³è¦æ›´å¤šæ•°æ®ï¼Œè¯·æŠŠè¿™é‡Œè®¾å¤§ (ä¾‹å¦‚ 800) æˆ–è€…è®¾ä¸º 0 (ä¸é™åˆ¶ï¼Œè·‘å®Œä¸ºæ­¢)
MAX_PROCESS_LIMIT = 500

# API é…ç½® (ä½ ç°åœ¨çš„ SiliconFlow é…ç½®)
API_KEY = "sk-mwphmyljrynungesqkaqnbimwghczzpniulmdgepgswhjrco" 
API_URL = "https://api.siliconflow.cn/v1/chat/completions"
MODEL_NAME = "Pro/zai-org/GLM-4.7"

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_DIR = os.path.join(BASE_DIR, 'database', 'raw')
OUTPUT_DIR = os.path.join(BASE_DIR, 'public', 'db', 'topic')

FILENAME_MAPPING = {
    "Taiwan": "Taiwan", "China_US": "US", "Philippines": "Philippines",
    "Japan": "Japan", "JP": "Japan", "ph": "Philippines", "us": "US", "jp": "Japan", "tw": "Taiwan"
}
# ===========================================

def get_files_fingerprint(date_key):
    target_date_str = date_key.replace("-", "") 
    related_files = []
    if os.path.exists(RAW_DIR):
        for root, dirs, files in os.walk(RAW_DIR):
            for f in files:
                if target_date_str in f and f.endswith('.json'):
                    try:
                        stat = os.stat(os.path.join(root, f))
                        related_files.append(f"{f}_{stat.st_size}")
                    except: pass
    return hashlib.md5("|".join(sorted(related_files)).encode('utf-8')).hexdigest()

def load_data_for_target_date(target_date):
    region_data = {}
    target_date_str = target_date.replace("-", "")
    if not os.path.exists(RAW_DIR): return region_data
    
    print(f"ğŸ“‚ æ‰«æåŸå§‹æ•°æ®: {target_date} ...")
    for root, dirs, files in os.walk(RAW_DIR):
        for filename in files:
            if not filename.endswith('.json') or target_date_str not in filename: continue
            
            target_region = None
            for key, region in FILENAME_MAPPING.items():
                if key.lower() in filename.lower(): 
                    target_region = region
                    break
            if not target_region: continue

            if target_region not in region_data: region_data[target_region] = []
            try:
                with open(os.path.join(root, filename), 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    items = data if isinstance(data, list) else [data]
                    for item in items:
                        if item.get('full_text'):
                            region_data[target_region].append(item)
            except: pass
    return region_data

def repair_json(json_str):
    json_str = json_str.strip()
    if not json_str.endswith(']') and not json_str.endswith('}'):
        if json_str.endswith(','): json_str = json_str[:-1]
        try: return json.loads(json_str + ']')
        except: 
            try: return json.loads(json_str + '}')
            except: pass
    try: return json.loads(json_str)
    except: return None

def call_llm(prompt, max_tokens=4096):
    try:
        response = requests.post(API_URL, json={
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": "You are a JSON generator. Output valid JSON only."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3,
            "max_tokens": max_tokens,
            "response_format": {"type": "json_object"}
        }, headers={"Authorization": f"Bearer {API_KEY}"}, timeout=120)
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            match = re.search(r'\{[\s\S]*\}|\[[\s\S]*\]', content)
            if match:
                return repair_json(match.group(0))
    except Exception as e:
        print(f"âš ï¸ LLM è°ƒç”¨å¼‚å¸¸: {e}")
    return None

def batch_process_tweets(tweets, region):
    """
    Step 1: åˆ†æ‰¹å…¨é‡å¤„ç†ã€‚
    """
    processed_results = []
    total_batches = math.ceil(len(tweets) / BATCH_SIZE)
    
    print(f"   [Step 1] æ­£åœ¨å¤„ç† {len(tweets)} æ¡æ¨æ–‡ï¼Œåˆ† {total_batches} æ‰¹æ‰§è¡Œ...")

    for i in range(0, len(tweets), BATCH_SIZE):
        batch = tweets[i : i + BATCH_SIZE]
        print(f"      -> å¤„ç†æ‰¹æ¬¡ {i//BATCH_SIZE + 1}/{total_batches} ...")
        
        input_txt = ""
        for idx, t in enumerate(batch):
            text = t.get('full_text', '').replace('\n', ' ').replace('"', "'")[:200]
            input_txt += f"ID[{idx}]: {text}\n"

        # æç¤ºè¯å¾®è°ƒï¼šè¦æ±‚å°½é‡ä¿ç•™æ•°æ®ï¼Œåªå‰”é™¤æ˜æ˜¾æ— æ„ä¹‰çš„
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªæƒ…æŠ¥åˆ†æåŠ©æ‰‹ã€‚è¯·å¤„ç†ä»¥ä¸‹æ¨æ–‡ã€‚
        
        {input_txt}

        ä»»åŠ¡ï¼š
        1. åˆ¤æ–­æ¨æ–‡æ˜¯å¦æœ‰å†…å®¹ (is_valid)ã€‚**ä¿ç•™æ‰€æœ‰åŒ…å«è§‚ç‚¹ã€æ–°é—»ã€äº‹å®çš„æ¨æ–‡**ï¼Œä»…å‰”é™¤çº¯ä¹±ç æˆ–çº¯å¹¿å‘Šã€‚
        2. ç¿»è¯‘æˆä¸­æ–‡ (translation)ã€‚å‡ºç°çš„æ¨ç‰¹ç”¨æˆ·åï¼ˆä¾‹å¦‚@Creed_is_T1)å¯ä»¥å¿½ç•¥ï¼Œä¸å‡ºç°åœ¨æ­£æ–‡ä¸­.
        3. åˆ¤æ–­è¿™ç¯‡æ¨æ–‡å¯¹ä¸­å›½çš„ç«‹åœº (stance): positive(äº²å)/neutral(ä¸­ç«‹)/negative(åå)ã€‚
        4. æå– 2-3 ä¸ªæ ¸å¿ƒä¸­æ–‡å…³é”®è¯ (keywords)ï¼Œç”¨äºåç»­èšç±»ã€‚

        è¾“å‡º JSON:
        {{
            "results": [
                {{ "id": 0, "is_valid": true, "translation": "...", "stance": "negative", "keywords": ["åŠå¯¼ä½“", "åˆ¶è£"] }},
                ...
            ]
        }}
        """
        
        res = call_llm(prompt)
        if res and 'results' in res:
            for item in res['results']:
                local_id = item.get('id')
                if local_id is not None and 0 <= local_id < len(batch):
                    if item.get('is_valid', True): 
                        original_tweet = batch[local_id]
                        processed_results.append({
                            "original_obj": original_tweet,
                            "translation": item.get('translation'),
                            "stance": item.get('stance'),
                            "keywords": item.get('keywords', [])
                        })
        
        # ç®€å•é˜²é™æµ
        time.sleep(0.5)

    return processed_results

def generate_topic_definitions(processed_tweets):
    """
    Step 2: è®© LLM åªç”Ÿæˆè¯é¢˜å®šä¹‰ï¼ˆä¸å½’ç±» IDï¼ŒèŠ‚çœ Tokenï¼‰ã€‚
    """
    if not processed_tweets: return [], []

    print(f"   [Step 2] æ­£åœ¨åˆ†æ {len(processed_tweets)} æ¡æ¨æ–‡çš„å…³é”®è¯ä»¥åˆ¶å®šè¯é¢˜...")

    # åªæå–å…³é”®è¯ç»™ LLM
    all_keywords = []
    for item in processed_tweets:
        all_keywords.extend(item['keywords'])
    
    # ä¸ºäº†é˜²æ­¢å…³é”®è¯å¤ªå¤šï¼Œæˆªå–å‰ 800 ä¸ªè¯ï¼ˆé€šå¸¸è¶³å¤Ÿä»£è¡¨æ•´ä½“è¶‹åŠ¿ï¼‰
    keywords_text = ", ".join(all_keywords[:800])

    prompt = f"""
    ä»¥ä¸‹æ˜¯å½“å‰ç¤¾äº¤åª’ä½“ä¸Šå…³äºæŸåœ°åŒºçš„çƒ­é—¨å…³é”®è¯é›†åˆï¼š
    {keywords_text}

    ä»»åŠ¡ï¼š
    1. æ ¹æ®è¿™äº›å…³é”®è¯ï¼Œæ€»ç»“å‡º 5-10 ä¸ªæ ¸å¿ƒèˆ†æƒ…è¯é¢˜ã€‚
    2. **è¯é¢˜åç§° (topic)** å¿…é¡»æ˜¯å…·ä½“çš„çŸ­è¯­ (å¦‚"å—æµ·å†²çª", "èŠ¯ç‰‡æ³•æ¡ˆ")ã€‚å°½é‡å°‘å‡ºç°å›½å®¶å(å¦‚â€œä¸­å›½ï¼Œç¾å›½â€)ï¼Œç”¨æ›´åŠ å…·ä½“çš„è¯è¯­æ¥æ›¿ä»£ï¼Œå¹¶ä¸”æ‰©å±•æˆä¸€ä¸ªç±»ä¼¼çƒ­æœçš„è¯é¢˜ï¼Œä¾‹å¦‚â€œå†¬å­£é£æš´é€ æˆè‡³å°‘30äººæ­»äº¡â€ã€‚
    3. ä¸ºæ¯ä¸ªè¯é¢˜æä¾› 3-5 ä¸ª**ä»£è¡¨æ€§å…³é”®è¯ (tags)**ï¼Œ**å…³é”®è¯ä¸è¦å‡ºç°å›½å®¶å(å¦‚ä¸­å›½ï¼Œç¾å›½)**ã€‚
    4. æå– Top 15 å…¨å±€çƒ­è¯ (hot_words)ã€‚

    è¾“å‡º JSON:
    {{
        "topics": [
            {{ "topic": "è¯é¢˜åç§°", "tags": ["å…³é”®è¯1", "å…³é”®è¯2"] }},
            ...
        ],
        "hot_words": [ {{ "name": "è¯", "value": 10 }} ]
    }}
    """

    res = call_llm(prompt)
    if not res: return [], []
    
    return res.get('topics', []), res.get('hot_words', [])

def classify_tweets_locally(processed_tweets, topic_definitions):
    """
    Step 3: æœ¬åœ° Python å½’ç±»ç®—æ³•ã€‚
    å¼ºåˆ¶å°† Step 1 çš„æ¨æ–‡åˆ†é…ç»™ Step 2 çš„è¯é¢˜ï¼Œä¿è¯æ•°æ®é‡ã€‚
    """
    print(f"   [Step 3] æ­£åœ¨æœ¬åœ°å½’ç±» {len(processed_tweets)} æ¡æ¨æ–‡...")
    
    # åˆå§‹åŒ–ç»“æœç»“æ„
    final_clusters = {t['topic']: {'topic': t['topic'], 'tweets': [], 'tags': t.get('tags', [])} for t in topic_definitions}
    # å¢åŠ ä¸€ä¸ªâ€œå…¶ä»–è¯é¢˜â€å…œåº•
    final_clusters["å…¶ä»–çƒ­ç‚¹"] = {'topic': "å…¶ä»–çƒ­ç‚¹", 'tweets': [], 'tags': []}

    for tweet in processed_tweets:
        best_topic = "å…¶ä»–çƒ­ç‚¹"
        max_score = 0
        
        # æ¨æ–‡çš„ç‰¹å¾ï¼šå®ƒçš„å…³é”®è¯ + ç¿»è¯‘æ–‡æœ¬
        tweet_text = (tweet['translation'] + " " + " ".join(tweet['keywords'])).lower()
        
        for topic in topic_definitions:
            score = 0
            # 1. åŒ¹é…è¯é¢˜åç§°
            if topic['topic'].lower() in tweet_text:
                score += 5
            # 2. åŒ¹é…è¯é¢˜æ ‡ç­¾
            for tag in topic.get('tags', []):
                if tag.lower() in tweet_text:
                    score += 2
            
            if score > max_score:
                max_score = score
                best_topic = topic['topic']
        
        # åªæœ‰åŒ¹é…åº¦å¤§äº0æ‰è¿›ç‰¹å®šè¯é¢˜ï¼Œå¦åˆ™è¿›â€œå…¶ä»–â€
        if max_score > 0:
            target_key = best_topic
        else:
            target_key = "å…¶ä»–çƒ­ç‚¹"

        # æ„é€ å‰ç«¯éœ€è¦çš„æ•°æ®æ ¼å¼
        orig = tweet['original_obj']
        tweet_data = {
            "text": orig.get('full_text', ''),
            "translation": tweet['translation'], 
            "stance": tweet['stance'],           
            "username": orig.get('username', 'Unknown'),
            "created_at": orig.get('created_at', ''),
            "metrics": {
                "reply": orig.get('reply_count', 0),
                "retweet": orig.get('retweet_count', 0),
                "like": orig.get('favorite_count', 0)
            }
        }
        final_clusters[target_key]['tweets'].append(tweet_data)

    # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶è¿‡æ»¤ç©ºè¯é¢˜
    sorted_topics = []
    # å…ˆæŠŠéâ€œå…¶ä»–â€çš„æŒ‰æ•°é‡æ’åº
    regular_topics = [v for k, v in final_clusters.items() if k != "å…¶ä»–çƒ­ç‚¹" and len(v['tweets']) > 0]
    regular_topics.sort(key=lambda x: len(x['tweets']), reverse=True)
    sorted_topics.extend(regular_topics)
    
    # æœ€åæ”¾â€œå…¶ä»–â€
    if len(final_clusters["å…¶ä»–çƒ­ç‚¹"]['tweets']) > 0:
        sorted_topics.append(final_clusters["å…¶ä»–çƒ­ç‚¹"])
        
    return sorted_topics

def calculate_stance_stats(topics):
    stats = {"positive": 0, "neutral": 0, "negative": 0}
    for t in topics:
        for tw in t['tweets']:
            s = str(tw.get('stance', 'neutral')).lower()
            if 'positive' in s or 'äº²å' in s: stats['positive'] += 1
            elif 'negative' in s or 'åå' in s: stats['negative'] += 1
            else: stats['neutral'] += 1
    return [
        {"name": "äº²å (Positive)", "value": stats['positive']},
        {"name": "ä¸­ç«‹ (Neutral)", "value": stats['neutral']},
        {"name": "åå (Negative)", "value": stats['negative']}
    ]

def main():
    print(f"ğŸš€ [æµ·é‡æ•°æ®æ¨¡å¼] å¼€å§‹æ‰§è¡Œ | æ—¥æœŸ: {TARGET_DATE}")
    print(f"âš™ï¸  é…ç½®: æ‰¹å¤§å°={BATCH_SIZE}, æœ€å¤§å¤„ç†é™åˆ¶={MAX_PROCESS_LIMIT} (0ä¸ºä¸é™)")
    
    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)
    out_path = os.path.join(OUTPUT_DIR, f"{TARGET_DATE}.json")

    current_data = {}
    if os.path.exists(out_path):
        try:
            with open(out_path, 'r', encoding='utf-8') as f:
                current_data = json.load(f)
        except: pass

    regions_map = load_data_for_target_date(TARGET_DATE)
    if not regions_map:
        print("âš ï¸ æœªæ‰¾åˆ°æºæ•°æ®")
        return

    print(f"\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    
    for region, items in regions_map.items():
        print(f"ğŸ”„ å¤„ç†æ¿å—: [{region}] (æºæ•°æ®å…± {len(items)} æ¡)...")
        
        # 1. é¢„è¿‡æ»¤
        unique_items = { (i.get('tweet_id') or i.get('full_text')): i for i in items }.values()
        clean_items = list(unique_items)
        if MAX_PROCESS_LIMIT > 0:
            clean_items = clean_items[:MAX_PROCESS_LIMIT]

        # 2. Step 1: LLM é€æ¡ç¿»è¯‘åˆ¤ç«‹
        processed = batch_process_tweets(clean_items, region)
        if not processed:
            print(f"   âŒ [{region}] æ— æœ‰æ•ˆæ¨æ–‡")
            continue

        # 3. Step 2: LLM å®šä¹‰è¯é¢˜
        topic_defs, hot_words = generate_topic_definitions(processed)

        # 4. Step 3: Python æœ¬åœ°å½’ç±» (ä¿è¯æ•°é‡)
        final_topics = classify_tweets_locally(processed, topic_defs)
        
        # 5. ç»Ÿè®¡
        stance_stats = calculate_stance_stats(final_topics)

        # 6. ä¿å­˜
        current_data[region] = {
            "region": region,
            "time_range": [TARGET_DATE, TARGET_DATE],
            "top_topics": final_topics,
            "hot_words": hot_words,
            "stance_stats": stance_stats,
            "total_analyzed": len(processed)
        }
        
        try:
            current_data["_meta"] = {
                "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            with open(out_path, 'w', encoding='utf-8') as f:
                json.dump(current_data, f, ensure_ascii=False, indent=2)
            
            total_tweets = sum(len(t['tweets']) for t in final_topics)
            print(f"   âœ… [{region}] å®Œæˆ: ç”Ÿæˆ {len(final_topics)} ä¸ªè¯é¢˜, åŒ…å« {total_tweets} æ¡æ¨æ–‡")
        except Exception as e:
            print(f"   âŒ [{region}] ä¿å­˜å¤±è´¥: {e}")

    print(f"\nğŸ‰ ä»»åŠ¡ç»“æŸ: {out_path}")

if __name__ == "__main__":
    main()