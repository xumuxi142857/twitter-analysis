import json
import os
import re
import requests
import hashlib
from datetime import datetime
import time

# ================= é…ç½®åŒºåŸŸ =================
# DeepSeek API
API_KEY = "sk-7ba052d40efe48ae990141e577d952d1"  # 
API_URL = "https://api.deepseek.com/chat/completions"
MODEL_NAME = "deepseek-chat"  # 

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_DIR = os.path.join(BASE_DIR, 'database', 'raw')
OUTPUT_DIR = os.path.join(BASE_DIR, 'public', 'db', 'topic')

FILENAME_MAPPING = {
    "Taiwan": "Taiwan",
    "China_US": "US", 
    "Philippines": "Philippines",
    "Japan": "Japan",
    "JP": "Japan",
    "ph": "Philippines",
    "us": "US",
    "jp": "Japan",
    "tw": "Taiwan"
}
# ===========================================

def parse_date_from_filename(filename):
    match = re.search(r'(\d{8})_(\d{6})', filename)
    if match:
        date_str = match.group(1)
        return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
    return None

def get_files_fingerprint(date_key):
    target_date_str = date_key.replace("-", "") 
    related_files = []
    if os.path.exists(RAW_DIR):
        for f in os.listdir(RAW_DIR):
            if target_date_str in f and f.endswith('.json'):
                path = os.path.join(RAW_DIR, f)
                stat = os.stat(path)
                related_files.append(f"{f}_{stat.st_size}_{stat.st_mtime}")
    if not related_files: return None
    related_files.sort()
    combined_str = "|".join(related_files)
    return hashlib.md5(combined_str.encode('utf-8')).hexdigest()

def check_needs_update(output_file, current_fingerprint):
    if not os.path.exists(output_file): return True
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            saved_fingerprint = data.get('_meta', {}).get('fingerprint', '')
            if saved_fingerprint != current_fingerprint: return True
            return False
    except: return True

def load_and_group_by_date():
    grouped_data = {}
    if not os.path.exists(RAW_DIR): return grouped_data

    for filename in os.listdir(RAW_DIR):
        if not filename.endswith('.json'): continue
        date_key = parse_date_from_filename(filename)
        if not date_key: continue
        target_region = None
        for key, region in FILENAME_MAPPING.items():
            if key.lower() in filename.lower(): 
                target_region = region
                break
        if not target_region: continue

        if date_key not in grouped_data: grouped_data[date_key] = {}
        if target_region not in grouped_data[date_key]: grouped_data[date_key][target_region] = []

        path = os.path.join(RAW_DIR, filename)
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                items = data if isinstance(data, list) else [data]
                
                for item in items:
                    if item.get('full_text'):
                        grouped_data[date_key][target_region].append(item)
        except: pass

    return grouped_data

def call_llm_analysis(region, raw_items):
    """
    raw_items: åŸå§‹æ¨æ–‡å¯¹è±¡åˆ—è¡¨
    """
    if not raw_items: return None

    # æ˜¾ç¤ºè¿›åº¦ï¼šå¼€å§‹å¤„ç†
    print(f"      ğŸ¤– å¼€å§‹åˆ†æ {region} æ¿å— ({len(raw_items)} æ¡æ¨æ–‡)...")
    
    # 1. æ„å»ºå¸¦ç´¢å¼•çš„è¾“å…¥ï¼Œæ–¹ä¾¿ LLM å¼•ç”¨
    # é™åˆ¶å‰ 60 æ¡ï¼Œé˜²æ­¢ Token æº¢å‡º
    process_items = raw_items[:60]
    input_list = []
    for idx, item in enumerate(process_items):
        text = item.get('full_text', '').replace('\n', ' ').strip()
        if len(text) > 10:
            input_list.append(f"ID[{idx}]: {text}")
    
    input_text_str = "\n".join(input_list)
    
    # 2. ä¿®æ”¹ Prompt
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæƒ…æŠ¥åˆ†æå‘˜ã€‚è¯·åˆ†æä»¥ä¸‹"{region}"æ¿å—çš„æ¨ç‰¹æ–‡æœ¬ã€‚
    
    æ–‡æœ¬åˆ—è¡¨ (å¸¦ID):
    {input_text_str}

    ä»»åŠ¡ï¼š
    1. ã€è¯é¢˜èšç±»ã€‘å°†æ¨æ–‡èšç±»ä¸º Top 10 æ ¸å¿ƒè¯é¢˜ã€‚
    2. ã€ç«‹åœºç ”åˆ¤ã€‘åˆ—å‡ºæ¯ä¸ªè¯é¢˜ä¸‹çš„æ¨æ–‡IDï¼Œå¹¶åˆ¤æ–­è¯¥æ¨æ–‡çš„ç«‹åœº(positive/neutral/negative)ã€‚
    3. ã€è¯äº‘æå–ã€‘æå– Top 20 çƒ­é—¨å…³é”®è¯ï¼Œå¿…é¡»ä¸¥æ ¼æŒ‰ç…§è¦æ±‚æ¥è¿›è¡Œæå–ã€‚
       è¯äº‘è¦æ±‚ï¼š
       - å¿…é¡»æ˜¯å…·ä½“çš„å®ä½“ã€äº‹ä»¶ã€åè¯ï¼ˆå¦‚"å…³ç¨"ã€"åä¸º"ã€"å—æµ·å†²çª"ï¼‰ã€‚
       - ç¦æ­¢è¾“å‡º"ä¸­å›½"ã€"ç¾å›½"ã€"China"ã€"US"ç­‰è¿‡äºå®½æ³›çš„å›½å®¶åç§°ã€‚
    
    è¾“å‡º JSON æ ¼å¼ï¼š
    {{
        "top_topics": [
            {{
                "topic": "è¯é¢˜æ‘˜è¦",
                "tweet_ids": [
                    {{"id": 0, "stance": "negative"}},
                    {{"id": 3, "stance": "neutral"}}
                ]
            }}
        ],
        "hot_words": [
            {{"name": "å…·ä½“åè¯", "value": 88}}
        ]
    }}
    """

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": "You are a data analyst. Output raw JSON only."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,
        "response_format": {"type": "json_object"}
    }
    
    try:
        # æ˜¾ç¤ºè¿›åº¦ï¼šæ­£åœ¨è°ƒç”¨ API
        print(f"      ğŸ“¡ æ­£åœ¨è°ƒç”¨ DeepSeek API...", end="", flush=True)
        start_time = time.time()
        
        response = requests.post(
            API_URL, 
            json=payload, 
            headers={
                "Authorization": f"Bearer {API_KEY}",
                "Content-Type": "application/json"
            },
            timeout=60  # æ·»åŠ è¶…æ—¶è®¾ç½®
        )
        
        # æ˜¾ç¤ºè¿›åº¦ï¼šAPI è°ƒç”¨å®Œæˆ
        api_time = time.time() - start_time
        print(f" å®Œæˆ (è€—æ—¶: {api_time:.1f}ç§’)")
        
        if response.status_code == 200:
            # æ˜¾ç¤ºè¿›åº¦ï¼šè§£æå“åº”
            print(f"      ğŸ” è§£æ API å“åº”...", end="", flush=True)
            
            llm_json = json.loads(response.json()['choices'][0]['message']['content'])
            
            # 3. ã€æ•°æ®å›å¡«ã€‘æ ¹æ® ID æŠŠåŸå§‹å…ƒæ•°æ®æ‹¼å›å»
            final_topics = []
            for topic_obj in llm_json.get('top_topics', []):
                enriched_tweets = []
                for t_ref in topic_obj.get('tweet_ids', []):
                    tid = t_ref.get('id')
                    stance = t_ref.get('stance', 'neutral')
                    
                    # ç¡®ä¿ ID æœ‰æ•ˆ
                    if isinstance(tid, int) and 0 <= tid < len(process_items):
                        original = process_items[tid]
                        enriched_tweets.append({
                            "text": original.get('full_text', ''),
                            "stance": stance,
                            # å›å¡«å…ƒæ•°æ®
                            "username": original.get('username', 'Unknown'),
                            "created_at": original.get('created_at', ''),
                            "metrics": {
                                "reply": original.get('reply_count', 0),
                                "retweet": original.get('retweet_count', 0),
                                "like": original.get('favorite_count', 0)
                            }
                        })
                
                if enriched_tweets:
                    final_topics.append({
                        "topic": topic_obj.get('topic'),
                        "tweets": enriched_tweets
                    })
            
            # æ˜¾ç¤ºè¿›åº¦ï¼šå¤„ç†å®Œæˆ
            print(f" å®Œæˆ")
            print(f"      âœ… {region} åˆ†æå®Œæˆ: {len(final_topics)} ä¸ªè¯é¢˜, {len(llm_json.get('hot_words', []))} ä¸ªçƒ­è¯")
            
            return {
                "top_topics": final_topics,
                "hot_words": llm_json.get('hot_words', [])
            }

        else:
            print(f"      âŒ API é”™è¯¯: {response.status_code} - {response.text}")
    except requests.exceptions.Timeout:
        print(f"      â° API è¯·æ±‚è¶…æ—¶")
    except Exception as e:
        print(f"      âš ï¸ å¼‚å¸¸: {e}")
    
    return None

def main():
    print("ğŸš€ å¼€å§‹æ‰§è¡Œè¯é¢˜æº¯æºåˆ†æ (Topic Drill-down & Metadata)...")
    
    # æ˜¾ç¤ºè¿›åº¦ï¼šæ£€æŸ¥ç›®å½•
    print("ğŸ“ æ£€æŸ¥ç›®å½•ç»“æ„...")
    if not os.path.exists(RAW_DIR):
        print(f"âŒ åŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {RAW_DIR}")
        return
    
    if not os.path.exists(OUTPUT_DIR):
        print(f"ğŸ“‚ åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        os.makedirs(OUTPUT_DIR)
    
    date_groups = load_and_group_by_date()
    
    if not date_groups:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å¯å¤„ç†çš„æ•°æ®æ–‡ä»¶")
        return
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(date_groups)} å¤©çš„æ•°æ®")
    
    total_dates = len(date_groups)
    processed_dates = 0
    
    for date_key, regions_data in date_groups.items():
        processed_dates += 1
        out_path = os.path.join(OUTPUT_DIR, f"{date_key}.json")
        current_fingerprint = get_files_fingerprint(date_key)
        
        print(f"\n{'='*50}")
        print(f"ğŸ“… å¤„ç†è¿›åº¦: {processed_dates}/{total_dates} | æ—¥æœŸ: {date_key}")
        print(f"{'='*50}")
        
        if not check_needs_update(out_path, current_fingerprint):
            print(f"â© æ•°æ®æœªå˜åŠ¨ï¼Œè·³è¿‡")
            continue
        
        print(f"ğŸ”„ å¼€å§‹èšç±»åˆ†æ...")
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        for region, items in regions_data.items():
            print(f"   â€¢ {region}: {len(items)} æ¡æ¨æ–‡")
        
        daily_result = {}
        
        total_regions = len(regions_data)
        processed_regions = 0
        
        for region, items in regions_data.items():
            processed_regions += 1
            print(f"\n   â”Œâ”€â”€ [{processed_regions}/{total_regions}] å¤„ç† {region} æ¿å—")
            analysis = call_llm_analysis(region, items)
            
            if analysis:
                daily_result[region] = {
                    "region": region,
                    "time_range": [date_key, date_key],
                    "top_topics": analysis.get('top_topics', []),
                    "hot_words": analysis.get('hot_words', [])
                }
            else:
                daily_result[region] = {"top_topics": [], "hot_words": []}
            
            print(f"   â””â”€â”€ å®Œæˆ")
        
        daily_result["_meta"] = {
            "fingerprint": current_fingerprint,
            "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "total_regions_processed": total_regions
        }

        with open(out_path, 'w', encoding='utf-8') as f:
            json.dump(daily_result, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… {date_key} æ›´æ–°æˆåŠŸ: {out_path}")

    print(f"\n{'ğŸ‰'*3} å…¨éƒ¨å¤„ç†å®Œæˆï¼ {'ğŸ‰'*3}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"ğŸ“… æ€»å…±å¤„ç†äº† {total_dates} å¤©çš„æ•°æ®")

if __name__ == "__main__":
    main()