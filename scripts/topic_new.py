import json
import os
import re
import requests
import hashlib
import sys
import time
import math
from datetime import datetime  # <--- è¡¥ä¸Šäº†è¿™ä¸ªå…³é”®çš„ import

# ================= é…ç½®åŒºåŸŸ =================
if len(sys.argv) > 1:
    TARGET_DATE = sys.argv[1]
else:
    TARGET_DATE = "2026-01-26"

# âš ï¸ æ‰¹å¤„ç†å¤§å°ï¼šæ¯æ‰¹å¤„ç† 15 æ¡
BATCH_SIZE = 15
# âš ï¸ æœ€å¤§å¤„ç†æ•°é‡ï¼šä¸Šé™ï¼Œè®¾ä¸º 0 åˆ™ä¸é™åˆ¶
MAX_PROCESS_LIMIT = 300

#API_KEY = "sk-7ba052d40efe48ae990141e577d952d1" 
#API_URL = "https://api.deepseek.com/chat/completions"
#MODEL_NAME = "deepseek-chat" 

API_KEY = "sk-mwphmyljrynungesqkaqnbimwghczzpniulmdgepgswhjrco" 
API_URL = "https://api.siliconflow.cn/v1/chat/completions"
MODEL_NAME = "Pro/zai-org/GLM-4.7" 

#API_KEY = "sk-jtDFyIPxnt2jqyHQPVsxiZwEcWOY2592WvEqN2F6tYP1juu6" 
#API_URL = "https://api.302.ai/v1/chat/completions"
#MODEL_NAME = "gpt-5-nano-2025-08-07" 

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_DIR = os.path.join(BASE_DIR, 'database', 'raw')
OUTPUT_DIR = os.path.join(BASE_DIR, 'public', 'db', 'topic')

FILENAME_MAPPING = {
    "Taiwan": "Taiwan", "China_US": "US", "Philippines": "Philippines",
    "Japan": "Japan", "JP": "Japan", "ph": "Philippines", "us": "US", "jp": "Japan", "tw": "Taiwan"
}
# ===========================================

def get_files_fingerprint(date_key):
    target_date_str = date_key.replace("-", "") 
    related_files = []
    if os.path.exists(RAW_DIR):
        for root, dirs, files in os.walk(RAW_DIR):
            for f in files:
                if target_date_str in f and f.endswith('.json'):
                    try:
                        stat = os.stat(os.path.join(root, f))
                        related_files.append(f"{f}_{stat.st_size}")
                    except: pass
    return hashlib.md5("|".join(sorted(related_files)).encode('utf-8')).hexdigest()

def load_data_for_target_date(target_date):
    region_data = {}
    target_date_str = target_date.replace("-", "")
    if not os.path.exists(RAW_DIR): return region_data
    
    print(f"ğŸ“‚ æ‰«æåŸå§‹æ•°æ®: {target_date} ...")
    for root, dirs, files in os.walk(RAW_DIR):
        for filename in files:
            if not filename.endswith('.json') or target_date_str not in filename: continue
            
            target_region = None
            for key, region in FILENAME_MAPPING.items():
                if key.lower() in filename.lower(): 
                    target_region = region
                    break
            if not target_region: continue

            if target_region not in region_data: region_data[target_region] = []
            try:
                with open(os.path.join(root, filename), 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    items = data if isinstance(data, list) else [data]
                    for item in items:
                        if item.get('full_text'):
                            region_data[target_region].append(item)
            except: pass
    return region_data

def repair_json(json_str):
    """å°è¯•ä¿®å¤æˆªæ–­çš„ JSON"""
    json_str = json_str.strip()
    if not json_str.endswith(']') and not json_str.endswith('}'):
        if json_str.endswith(','): json_str = json_str[:-1]
        try: return json.loads(json_str + ']')
        except: 
            try: return json.loads(json_str + '}')
            except: pass
    try: return json.loads(json_str)
    except: return None

def call_llm(prompt, max_tokens=4096):
    try:
        response = requests.post(API_URL, json={
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": "You are a JSON generator. Output valid JSON only."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3,
            "max_tokens": max_tokens,
            "response_format": {"type": "json_object"}
        }, headers={"Authorization": f"Bearer {API_KEY}"}, timeout=120)
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            match = re.search(r'\{[\s\S]*\}|\[[\s\S]*\]', content)
            if match:
                return repair_json(match.group(0))
    except Exception as e:
        print(f"âš ï¸ LLM è°ƒç”¨å¼‚å¸¸: {e}")
    return None

def batch_process_tweets(tweets, region):
    """
    ç¬¬ä¸€é˜¶æ®µï¼šåˆ†æ‰¹å¤„ç†ã€‚ç¿»è¯‘ã€åˆ¤ç«‹ã€æå–å…³é”®è¯ã€è¿‡æ»¤åƒåœ¾ã€‚
    """
    processed_results = []
    total_batches = math.ceil(len(tweets) / BATCH_SIZE)
    
    print(f"   [Step 1] æ­£åœ¨å¤„ç† {len(tweets)} æ¡æ¨æ–‡ï¼Œåˆ† {total_batches} æ‰¹æ‰§è¡Œ...")

    for i in range(0, len(tweets), BATCH_SIZE):
        batch = tweets[i : i + BATCH_SIZE]
        print(f"      -> å¤„ç†æ‰¹æ¬¡ {i//BATCH_SIZE + 1}/{total_batches} ...")
        
        input_txt = ""
        for idx, t in enumerate(batch):
            text = t.get('full_text', '').replace('\n', ' ').replace('"', "'")[:200]
            input_txt += f"ID[{idx}]: {text}\n"

        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªæƒ…æŠ¥åˆ†æåŠ©æ‰‹ã€‚è¯·å¤„ç†ä»¥ä¸‹æ¨æ–‡åˆ—è¡¨ã€‚
        
        {input_txt}

        ä»»åŠ¡ï¼š
        1. åˆ¤æ–­æ¨æ–‡æ˜¯å¦æœ‰æƒ…æŠ¥ä»·å€¼ (is_valid)ã€‚è·³è¿‡çº¯å¹¿å‘Šã€ä¹±ç æˆ–æ— å…³å†…å®¹ã€‚
        2. ç¿»è¯‘æˆä¸­æ–‡ (translation) å‡ºç°çš„æ¨ç‰¹ç”¨æˆ·åï¼ˆä¾‹å¦‚@Creed_is_T1ï¼Œå¯ä»¥å¿½ç•¥ï¼Œä¸å‡ºç°åœ¨æ­£æ–‡ä¸­ï¼‰ã€‚
        3. åˆ¤æ–­å¯¹åç«‹åœº (stance): positive(äº²å)/neutral(ä¸­ç«‹)/negative(åå)ã€‚
        4. æå– 2-3 ä¸ªæ ¸å¿ƒå…³é”®è¯æˆ–çŸ­è¯­ (keywords)ï¼Œå¿…é¡»æ˜¯åŸæ–‡ä¸­å‡ºç°çš„é«˜é¢‘è¯æ±‡ç¿»è¯‘æˆçš„ä¸­æ–‡ã€‚

        è¾“å‡º JSON åˆ—è¡¨:
        {{
            "results": [
                {{ "id": 0, "is_valid": true, "translation": "...", "stance": "negative", "keywords": ["è´¸æ˜“æˆ˜", "å…³ç¨"] }},
                ...
            ]
        }}
        """
        
        res = call_llm(prompt)
        if res and 'results' in res:
            for item in res['results']:
                local_id = item.get('id')
                if local_id is not None and 0 <= local_id < len(batch):
                    if item.get('is_valid', True): 
                        original_tweet = batch[local_id]
                        processed_results.append({
                            "original_obj": original_tweet,
                            "translation": item.get('translation'),
                            "stance": item.get('stance'),
                            "keywords": item.get('keywords', [])
                        })
        
        time.sleep(1) 

    return processed_results

def global_cluster_topics(processed_tweets, region):
    """
    ç¬¬äºŒé˜¶æ®µï¼šåŸºäºæ‰€æœ‰æå–å‡ºçš„å…³é”®è¯è¿›è¡Œèšç±»ã€‚
    """
    if not processed_tweets: return [], []

    print(f"   [Step 2] æ­£åœ¨å¯¹ {len(processed_tweets)} æ¡æœ‰æ•ˆæ¨æ–‡è¿›è¡Œå…¨å±€èšç±»...")

    # å‡†å¤‡èšç±»è¾“å…¥ï¼šåªå‘é€ ID å’Œ å…³é”®è¯
    cluster_input = ""
    for idx, item in enumerate(processed_tweets):
        kws = ", ".join(item['keywords'])
        cluster_input += f"GID[{idx}]: {kws}\n"

    prompt = f"""
    ä»¥ä¸‹æ˜¯å¤šæ¡æ¨æ–‡çš„å…³é”®è¯åˆ—è¡¨ã€‚è¯·æ ¹æ®è¿™äº›å…³é”®è¯å°†æ¨æ–‡å½’ç±»ä¸º äº”åˆ°å ä¸ªæ ¸å¿ƒè¯é¢˜ã€‚

    {cluster_input}

    æ ¸å¿ƒè¦æ±‚ï¼š
    1. **è¯é¢˜åç§°(topic)** å¿…é¡»æ˜¯å…·ä½“çš„ã€åœ¨æ¨æ–‡ä¸­å‡ºç°è¿‡çš„**é«˜é¢‘çŸ­è¯­** (å¦‚"åŠå¯¼ä½“åˆ¶è£", "æµ·è­¦èˆ¹ç¢°æ’")ï¼Œå°½é‡å°‘å‡ºç°å›½å®¶å(å¦‚â€œä¸­å›½ï¼Œç¾å›½â€)ï¼Œç”¨æ›´åŠ å…·ä½“çš„è¯è¯­æ¥æ›¿ä»£ï¼Œå¹¶ä¸”æ‰©å±•æˆä¸€ä¸ªç±»ä¼¼çƒ­æœçš„è¯é¢˜ï¼Œä¾‹å¦‚â€œå†¬å­£é£æš´é€ æˆè‡³å°‘30äººæ­»äº¡â€ã€‚
    2. æ¯ä¸ªæ¨æ–‡ (GID) åªèƒ½å½’å…¥ä¸€ä¸ªæœ€åŒ¹é…çš„è¯é¢˜ã€‚
    3. åŒæ—¶æå–æ•´ä¸ªæ•°æ®é›†çš„ Top 15 çƒ­é—¨è¯äº‘ (hot_words)ã€‚

    è¾“å‡º JSON:
    {{
        "topics": [
            {{ "topic": "å…·ä½“çŸ­è¯­", "gids": [0, 5, 12...] }},
            ...
        ],
        "hot_words": [ {{ "name": "è¯", "value": 10 }} ]
    }}
    """

    res = call_llm(prompt)
    if not res: return [], []

    final_topics = []
    used_gids = set()

    for topic_obj in res.get('topics', []):
        topic_name = topic_obj.get('topic', 'æœªå‘½åè¯é¢˜')
        tweets_in_topic = []
        
        for gid in topic_obj.get('gids', []):
            if isinstance(gid, int) and 0 <= gid < len(processed_tweets):
                pt = processed_tweets[gid]
                orig = pt['original_obj']
                tweets_in_topic.append({
                    "text": orig.get('full_text', ''),
                    "translation": pt['translation'], 
                    "stance": pt['stance'],           
                    "username": orig.get('username', 'Unknown'),
                    "created_at": orig.get('created_at', ''),
                    "metrics": {
                        "reply": orig.get('reply_count', 0),
                        "retweet": orig.get('retweet_count', 0),
                        "like": orig.get('favorite_count', 0)
                    }
                })
                used_gids.add(gid)
        
        if tweets_in_topic:
            final_topics.append({
                "topic": topic_name,
                "tweets": tweets_in_topic
            })
    
    return final_topics, res.get('hot_words', [])

def calculate_stance_stats(topics):
    """ç»Ÿè®¡å…¨æ¿å—çš„ç«‹åœºåˆ†å¸ƒ"""
    stats = {"positive": 0, "neutral": 0, "negative": 0}
    for t in topics:
        for tw in t['tweets']:
            s = str(tw.get('stance', 'neutral')).lower()
            if 'positive' in s or 'äº²å' in s: stats['positive'] += 1
            elif 'negative' in s or 'åå' in s: stats['negative'] += 1
            else: stats['neutral'] += 1
    
    return [
        {"name": "äº²å (Positive)", "value": stats['positive']},
        {"name": "ä¸­ç«‹ (Neutral)", "value": stats['neutral']},
        {"name": "åå (Negative)", "value": stats['negative']}
    ]

def main():
    print(f"ğŸš€ [å…¨é‡åˆ†ææ¨¡å¼] å¼€å§‹æ‰§è¡Œ | æ—¥æœŸ: {TARGET_DATE}")
    
    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)
    out_path = os.path.join(OUTPUT_DIR, f"{TARGET_DATE}.json")

    current_data = {}
    if os.path.exists(out_path):
        try:
            with open(out_path, 'r', encoding='utf-8') as f:
                current_data = json.load(f)
        except: pass

    regions_map = load_data_for_target_date(TARGET_DATE)
    if not regions_map:
        print("âš ï¸ æœªæ‰¾åˆ°æºæ•°æ®")
        return

    print(f"\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    
    for region, items in regions_map.items():
        print(f"ğŸ”„ å¤„ç†æ¿å—: [{region}] (å…± {len(items)} æ¡)...")
        
        # 1. é¢„è¿‡æ»¤ (å»é‡ & é™åˆ¶æ•°é‡)
        unique_items = { (i.get('tweet_id') or i.get('full_text')): i for i in items }.values()
        clean_items = list(unique_items)
        if MAX_PROCESS_LIMIT > 0:
            clean_items = clean_items[:MAX_PROCESS_LIMIT]

        # 2. ç¬¬ä¸€æ­¥ï¼šåˆ†æ‰¹æ¬¡å…¨é‡åˆ†æ
        processed = batch_process_tweets(clean_items, region)
        
        if not processed:
            print(f"   âŒ [{region}] æ— æœ‰æ•ˆæ¨æ–‡ï¼Œè·³è¿‡")
            continue

        # 3. ç¬¬äºŒæ­¥ï¼šå…¨å±€èšç±»
        topics, hot_words = global_cluster_topics(processed, region)
        
        # 4. è®¡ç®—ç»Ÿè®¡æ•°æ®
        stance_chart_data = calculate_stance_stats(topics)

        # 5. ä¿å­˜ (æ¯åšä¸€ä¸ªæ¿å—å°±å­˜ä¸€æ¬¡)
        current_data[region] = {
            "region": region,
            "time_range": [TARGET_DATE, TARGET_DATE],
            "top_topics": topics,
            "hot_words": hot_words,
            "stance_stats": stance_chart_data, 
            "total_analyzed": len(processed)
        }
        
        # å®‰å…¨ä¿å­˜ï¼šè¿™é‡Œä½¿ç”¨äº† try-except é˜²æ­¢ä¿å­˜æ—¶å´©æºƒå¯¼è‡´æ•°æ®ä¸¢å¤±
        try:
            current_data["_meta"] = {
                "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            with open(out_path, 'w', encoding='utf-8') as f:
                json.dump(current_data, f, ensure_ascii=False, indent=2)
            print(f"   âœ… [{region}] å®Œæˆ: {len(topics)} ä¸ªè¯é¢˜, åˆ†æäº† {len(processed)} æ¡æ¨æ–‡")
        except Exception as e:
            print(f"   âŒ [{region}] ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            # å°è¯•å¤‡ä»½ä¿å­˜
            with open(f"{out_path}.bak", 'w', encoding='utf-8') as f:
                json.dump(current_data, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ ä»»åŠ¡ç»“æŸ: {out_path}")

if __name__ == "__main__":
    main()