import json
import os
import re
import requests
import hashlib
from datetime import datetime

# ================= é…ç½®åŒºåŸŸ =================
API_KEY = "sk-mwphmyljrynungesqkaqnbimwghczzpniulmdgepgswhjrco" 
API_URL = "https://api.siliconflow.cn/v1/chat/completions"
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_DIR = os.path.join(BASE_DIR, 'database1', 'raw')
OUTPUT_DIR = os.path.join(BASE_DIR, 'public', 'db', 'topic')

FILENAME_MAPPING = {
    "Taiwan": "Taiwan",
    "US": "US", 
    "Philippines": "Philippines",
    "Japan": "Japan",
    "JP": "Japan",
    "ph": "Philippines",
    "us": "US",
    "jp": "Japan",
    "tw": "Taiwan"
}
# ===========================================

def parse_date_from_filename(filename):
    match = re.search(r'(\d{8})_(\d{6})', filename)
    if match:
        date_str = match.group(1)
        return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
    return None

def get_files_fingerprint(date_key):
    """
    è®¡ç®—è¯¥æ—¥æœŸä¸‹æ‰€æœ‰ç›¸å…³æ–‡ä»¶çš„æŒ‡çº¹ã€‚
    """
    target_date_str = date_key.replace("-", "") 
    related_files = []
    
    if os.path.exists(RAW_DIR):
        for root, dirs, files in os.walk(RAW_DIR):
            for f in files:
                if target_date_str in f and f.endswith('.json'):
                    path = os.path.join(root, f)
                    stat = os.stat(path)
                    # åŒ…å«æ–‡ä»¶è·¯å¾„ã€å¤§å°ã€ä¿®æ”¹æ—¶é—´ï¼Œç¡®ä¿å”¯ä¸€æ€§
                    related_files.append(f"{f}_{stat.st_size}_{stat.st_mtime}")
    
    if not related_files: return None
    related_files.sort()
    combined_str = "|".join(related_files)
    return hashlib.md5(combined_str.encode('utf-8')).hexdigest()

def check_needs_update(output_file, current_fingerprint):
    if not os.path.exists(output_file): return True
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            saved_fingerprint = data.get('_meta', {}).get('fingerprint', '')
            return saved_fingerprint != current_fingerprint
    except: return True

def load_and_group_by_date():
    """
    ã€å…³é”®é€»è¾‘ã€‘
    éå†æ–‡ä»¶å¤¹ï¼Œå°†æ•°æ®æ•´ç†ä¸º:
    grouped_data[æ—¥æœŸ][åœ°åŒº] = [æ¨æ–‡åˆ—è¡¨]
    """
    grouped_data = {}
    if not os.path.exists(RAW_DIR): return grouped_data

    print(f"ğŸ“‚ æ­£åœ¨æ‰«æ {RAW_DIR} åŠå…¶å­ç›®å½•...")
    
    file_count = 0
    # ä½¿ç”¨ os.walk é€’å½’æ‰«ææ‰€æœ‰å­æ–‡ä»¶å¤¹
    for root, dirs, files in os.walk(RAW_DIR):
        for filename in files:
            if not filename.endswith('.json'): continue
            
            # 1. è¯†åˆ«æ—¥æœŸ
            date_key = parse_date_from_filename(filename)
            if not date_key: continue
            
            # 2. è¯†åˆ«åœ°åŒº
            target_region = None
            for key, region in FILENAME_MAPPING.items():
                if key.lower() in filename.lower(): 
                    target_region = region
                    break
            if not target_region: continue

            # 3. åˆå§‹åŒ–å­—å…¸ç»“æ„
            if date_key not in grouped_data: grouped_data[date_key] = {}
            if target_region not in grouped_data[date_key]: grouped_data[date_key][target_region] = []

            # 4. è¯»å–æ•°æ®å¹¶å½’ç±»
            path = os.path.join(root, filename)
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    items = data if isinstance(data, list) else [data]
                    
                    for item in items:
                        if item.get('full_text'):
                            # å°†æ¨æ–‡æ”¾å…¥ [ç‰¹å®šæ—¥æœŸ][ç‰¹å®šåœ°åŒº] çš„åˆ—è¡¨ä¸­
                            grouped_data[date_key][target_region].append(item)
                    file_count += 1
            except: pass
            
    print(f"âœ… æ‰«æå®Œæˆï¼Œå…±è¯†åˆ« {file_count} ä¸ªæœ‰æ•ˆæ•°æ®æ–‡ä»¶")
    return grouped_data

def call_llm_analysis(region, date, raw_items):
    """
    ã€æ™ºèƒ½é‡‡æ ·ã€‘
    è¾“å…¥: ä»…åŒ…å«æŸä¸€å¤©ã€æŸä¸€ä¸ªåœ°åŒºçš„ raw_items
    è¾“å‡º: åˆ†æç»“æœ
    """
    if not raw_items: return None

    # 1. å»é‡ (æŒ‰æ¨æ–‡ID æˆ– æ–‡æœ¬)
    unique_items = {}
    for item in raw_items:
        key = item.get('tweet_id') or item.get('full_text')
        unique_items[key] = item
    clean_items = list(unique_items.values())

    # 2. æŒ‰å½±å“åŠ›æ’åº (Top-N ç­–ç•¥)
    # å½±å“åŠ› = è½¬å‘*2 + å›å¤ + ç‚¹èµ*0.5
    def calculate_impact(item):
        retweet = item.get('retweet_count', 0) or 0
        reply = item.get('reply_count', 0) or 0
        like = item.get('favorite_count', 0) or 0
        return (retweet * 2) + (reply * 1) + (like * 0.5)

    clean_items.sort(key=calculate_impact, reverse=True)

    # 3. æˆªå– Top 100
    # è¿™ç¡®ä¿äº†æˆ‘ä»¬åªåˆ†æè¿™ä¸€å¤©è¿™ä¸ªåœ°åŒºæœ€ç«çš„ 100 æ¡æ¨æ–‡
    top_items = clean_items[:100]
    
    # æ‰“å°æ—¥å¿—è¯æ˜é€»è¾‘æ˜¯æ­£ç¡®çš„
    print(f"      [é‡‡æ ·æ—¥å¿—] {date} | {region}: åŸå§‹ {len(raw_items)} æ¡ -> ç²¾é€‰ Top {len(top_items)} æ¡")

    # 4. æ„å»º Prompt è¾“å…¥
    input_list = []
    for idx, item in enumerate(top_items):
        text = item.get('full_text', '').replace('\n', ' ').strip()
        if len(text) > 15:
            input_list.append(f"ID[{idx}]: {text}")
    
    input_text_str = "\n".join(input_list)
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæƒ…æŠ¥åˆ†æå‘˜ã€‚è¯·åˆ†æä»¥ä¸‹â€œ{region}â€æ¿å—çš„æ¨ç‰¹æ–‡æœ¬ã€‚
    
    æ–‡æœ¬åˆ—è¡¨ (å¸¦ID):
    {input_text_str}

    ä»»åŠ¡ï¼š
    1. ã€è¯é¢˜èšç±»ã€‘å°†æ¨æ–‡èšç±»ä¸º Top 10 æ ¸å¿ƒè¯é¢˜ã€‚
    2. ã€ç«‹åœºç ”åˆ¤ã€‘åˆ—å‡ºæ¯ä¸ªè¯é¢˜ä¸‹çš„æ¨æ–‡IDï¼Œå¹¶åˆ¤æ–­è¯¥æ¨æ–‡çš„ç«‹åœº(positive/neutral/negative)ã€‚
    3. ã€è¯äº‘æå–ã€‘æå– Top 20 çƒ­é—¨å…³é”®è¯ (æ’é™¤é€šç”¨å›½å®¶åï¼Œåªä¿ç•™å…·ä½“äº‹ä»¶/å®ä½“)ã€‚
    
    è¾“å‡º JSON æ ¼å¼ï¼š
    {{
        "top_topics": [
            {{
                "topic": "è¯é¢˜æ‘˜è¦",
                "tweet_ids": [
                    {{"id": 0, "stance": "negative"}},
                    {{"id": 3, "stance": "neutral"}}
                ]
            }}
        ],
        "hot_words": [
            {{"name": "å…·ä½“åè¯", "value": 88}}
        ]
    }}
    """

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": "You are a data analyst. Output raw JSON only."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,
        "response_format": {"type": "json_object"}
    }
    
    try:
        response = requests.post(API_URL, json=payload, headers={
            "Authorization": f"Bearer {API_KEY}",
            "Content-Type": "application/json"
        })
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            llm_json = json.loads(content)
            
            # æ•°æ®å›å¡«
            final_topics = []
            for topic_obj in llm_json.get('top_topics', []):
                enriched_tweets = []
                for t_ref in topic_obj.get('tweet_ids', []):
                    tid = t_ref.get('id')
                    stance = t_ref.get('stance', 'neutral')
                    
                    if isinstance(tid, int) and 0 <= tid < len(top_items):
                        original = top_items[tid]
                        enriched_tweets.append({
                            "text": original.get('full_text', ''),
                            "stance": stance,
                            "username": original.get('username', 'Unknown'),
                            "created_at": original.get('created_at', ''),
                            "metrics": {
                                "reply": original.get('reply_count', 0),
                                "retweet": original.get('retweet_count', 0),
                                "like": original.get('favorite_count', 0)
                            }
                        })
                
                if enriched_tweets:
                    final_topics.append({
                        "topic": topic_obj.get('topic'),
                        "tweets": enriched_tweets
                    })
            
            return {
                "top_topics": final_topics,
                "hot_words": llm_json.get('hot_words', [])
            }
        else:
            print(f"âš ï¸ API Error ({region}): {response.status_code}")
    except Exception as e:
        print(f"âš ï¸ Exception ({region}): {e}")
    
    return None

def main():
    print("ğŸš€ å¼€å§‹æ‰§è¡Œè¯é¢˜æº¯æºåˆ†æ (Topic Drill-down & Metadata)...")
    
    # 1. å…ˆæŒ‰æ—¥æœŸå’Œåœ°åŒºåˆ†ç»„
    date_groups = load_and_group_by_date()
    
    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)

    # 2. éå†æ¯ä¸€ä¸ªæ—¥æœŸ (ä¾‹å¦‚ 2025-12-25)
    for date_key, regions_data in date_groups.items():
        out_path = os.path.join(OUTPUT_DIR, f"{date_key}.json")
        current_fingerprint = get_files_fingerprint(date_key)
        
        # æ™ºèƒ½è·³è¿‡é€»è¾‘
        if not check_needs_update(out_path, current_fingerprint):
            print(f"â© æ—¥æœŸ {date_key} æœªå˜åŠ¨ï¼Œè·³è¿‡")
            continue

        print(f"\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"ğŸ”„ æ­£åœ¨å¤„ç†æ—¥æœŸ: {date_key}")
        
        daily_result = {}
        
        # 3. éå†è¯¥æ—¥æœŸä¸‹çš„æ¯ä¸€ä¸ªåœ°åŒº (ä¾‹å¦‚ US, Philippines)
        for region, items in regions_data.items():
            print(f"   -> æ­£åœ¨åˆ†æ [{region}] æ¿å—...")
            
            # è¿™é‡Œçš„ items åªæ˜¯å½“å¤©çš„ã€è¯¥åœ°åŒºçš„æ•°æ®
            analysis = call_llm_analysis(region, date_key, items)
            
            if analysis:
                daily_result[region] = {
                    "region": region,
                    "time_range": [date_key, date_key],
                    "top_topics": analysis.get('top_topics', []),
                    "hot_words": analysis.get('hot_words', [])
                }
            else:
                daily_result[region] = {"top_topics": [], "hot_words": []}
        
        daily_result["_meta"] = {
            "fingerprint": current_fingerprint,
            "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        with open(out_path, 'w', encoding='utf-8') as f:
            json.dump(daily_result, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ—¥æœŸ {date_key} æ›´æ–°æˆåŠŸ")

    print("\nğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()