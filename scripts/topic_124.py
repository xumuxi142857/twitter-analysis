import json
import os
import re
import requests
import hashlib
from datetime import datetime

# ================= é…ç½®åŒºåŸŸ =================
# ğŸ“… æŒ‡å®šæ—¥æœŸ
TARGET_DATE = "2025-12-25" 

API_KEY = "sk-7ba052d40efe48ae990141e577d952d1" 
API_URL = "https://api.deepseek.com/chat/completions"
MODEL_NAME = "deepseek-chat" 

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_DIR = os.path.join(BASE_DIR, 'database', 'raw')
OUTPUT_DIR = os.path.join(BASE_DIR, 'public', 'db', 'topic')

FILENAME_MAPPING = {
    "Taiwan": "Taiwan",
    "China_US": "US", 
    "Philippines": "Philippines",
    "Japan": "Japan",
    "JP": "Japan",
    "ph": "Philippines",
    "us": "US",
    "jp": "Japan",
    "tw": "Taiwan"
}
# ===========================================

def get_files_fingerprint(date_key):
    """è®¡ç®—ç›®æ ‡æ—¥æœŸä¸‹ç›¸å…³æ–‡ä»¶çš„æŒ‡çº¹"""
    target_date_str = date_key.replace("-", "") 
    related_files = []
    
    if os.path.exists(RAW_DIR):
        for root, dirs, files in os.walk(RAW_DIR):
            for f in files:
                if target_date_str in f and f.endswith('.json'):
                    path = os.path.join(root, f)
                    stat = os.stat(path)
                    related_files.append(f"{f}_{stat.st_size}_{stat.st_mtime}")
    
    if not related_files: return None
    related_files.sort()
    combined_str = "|".join(related_files)
    return hashlib.md5(combined_str.encode('utf-8')).hexdigest()

def load_data_for_target_date(target_date):
    """åªåŠ è½½æŒ‡å®šæ—¥æœŸçš„æ•°æ®"""
    region_data = {}
    target_date_str = target_date.replace("-", "")
    
    if not os.path.exists(RAW_DIR):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°ç›®å½• {RAW_DIR}")
        return region_data

    print(f"ğŸ“‚ æ­£åœ¨æ‰«æ {RAW_DIR} ä¸­åŒ…å« '{target_date_str}' çš„æ–‡ä»¶...")
    file_count = 0
    
    for root, dirs, files in os.walk(RAW_DIR):
        for filename in files:
            if not filename.endswith('.json'): continue
            if target_date_str not in filename: continue
            
            target_region = None
            for key, region in FILENAME_MAPPING.items():
                if key.lower() in filename.lower(): 
                    target_region = region
                    break
            if not target_region: continue

            if target_region not in region_data: region_data[target_region] = []

            path = os.path.join(root, filename)
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    items = data if isinstance(data, list) else [data]
                    for item in items:
                        if item.get('full_text'):
                            region_data[target_region].append(item)
                    file_count += 1
            except: pass
            
    print(f"âœ… æ‰«æå®Œæˆï¼Œå…±æ‰¾åˆ° {file_count} ä¸ªç›¸å…³æ–‡ä»¶")
    return region_data

def call_llm_analysis(region, date, raw_items):
    """
    æ™ºèƒ½é‡‡æ · + LLM åˆ†æ (åŒ…å«ç¿»è¯‘)
    """
    if not raw_items: return None

    # 1. å»é‡
    unique_items = {}
    for item in raw_items:
        key = item.get('tweet_id') or item.get('full_text')
        unique_items[key] = item
    clean_items = list(unique_items.values())

    # 2. æŒ‰å½±å“åŠ›æ’åº
    def calculate_impact(item):
        retweet = item.get('retweet_count', 0) or 0
        reply = item.get('reply_count', 0) or 0
        like = item.get('favorite_count', 0) or 0
        return (retweet * 2) + (reply * 1) + (like * 0.5)

    clean_items.sort(key=calculate_impact, reverse=True)

    # 3. æˆªå– Top 50
    top_items = clean_items[:50]
    print(f"      [é‡‡æ ·] {region}: åŸå§‹ {len(raw_items)} æ¡ -> ç²¾é€‰ Top {len(top_items)} æ¡")

    # 4. æ„å»ºè¾“å…¥
    input_list = []
    for idx, item in enumerate(top_items):
        text = item.get('full_text', '').replace('\n', ' ').strip()
        if len(text) > 15:
            input_list.append(f"ID[{idx}]: {text}")
    
    input_text_str = "\n".join(input_list)
    
    # 5. æ„å»º Prompt (æ ¸å¿ƒä¿®æ”¹ï¼šè¦æ±‚ç¿»è¯‘ã€è¯é¢˜éš”ç¦»ã€åŠ¨æ€æ•°é‡)
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæƒ…æŠ¥åˆ†æå‘˜ã€‚è¯·åˆ†æä»¥ä¸‹â€œ{region}â€æ¿å—çš„æ¨ç‰¹æ–‡æœ¬ã€‚
    
    æ–‡æœ¬åˆ—è¡¨ (å¸¦ID):
    {input_text_str}

    ä»»åŠ¡ï¼š
    1. ã€è¯é¢˜èšç±»ã€‘è¯†åˆ« 5 åˆ° 10 ä¸ªæ ¸å¿ƒèˆ†æƒ…è¯é¢˜ã€‚
       - è¦æ±‚ï¼šè¯é¢˜ä¹‹é—´å¿…é¡»æœ‰æ˜æ˜¾çš„åŒºåˆ†åº¦ï¼ˆIsolationï¼‰ï¼Œä¸¥ç¦è¯é¢˜å«ä¹‰é‡å¤æˆ–åŒ…å«ã€‚
       - æ•°é‡ï¼šæ ¹æ®å†…å®¹ä¸°å¯Œåº¦åŠ¨æ€å†³å®šï¼Œä¸å¿…å¼ºåˆ¶å‡‘å¤Ÿ10ä¸ªï¼Œä½†è‡³å°‘5ä¸ªã€‚
    2. ã€æ¨æ–‡ç ”åˆ¤ã€‘å°†ç›¸å…³æ¨æ–‡å½’ç±»åˆ°å¯¹åº”è¯é¢˜ä¸‹ã€‚
       - å¯¹äºæ¯ä¸€æ¡å½’ç±»çš„æ¨æ–‡ï¼Œå¿…é¡»æä¾›ï¼š
         a) å…·ä½“çš„ç«‹åœºåˆ¤è¯» (positive/neutral/negative)
         b) æµç•…å‡†ç¡®çš„ä¸­æ–‡ç¿»è¯‘ (Translation)
    3. ã€è¯äº‘æå–ã€‘æå– Top 20 çƒ­é—¨å…³é”®è¯ (æ’é™¤é€šç”¨å›½å®¶åï¼Œåªä¿ç•™å…·ä½“äº‹ä»¶/å®ä½“)ï¼Œå¹¶ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚
    
    è¾“å‡º JSON æ ¼å¼ï¼ˆä¸¥ç¦ä½¿ç”¨Markdownï¼Œç›´æ¥è¾“å‡ºJSONï¼‰ï¼š
    {{
        "top_topics": [
            {{
                "topic": "è¯é¢˜æ‘˜è¦(ä¸­æ–‡)",
                "tweet_ids": [
                    {{"id": 0, "stance": "negative", "translation": "è¿™é‡Œæ˜¯æ¨æ–‡0çš„ä¸­æ–‡ç¿»è¯‘..."}},
                    {{"id": 3, "stance": "neutral", "translation": "è¿™é‡Œæ˜¯æ¨æ–‡3çš„ä¸­æ–‡ç¿»è¯‘..."}}
                ]
            }}
        ],
        "hot_words": [
            {{"name": "å…³é”®è¯(ä¸­æ–‡)", "value": 88}}
        ]
    }}
    """

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": "You are a data analyst. Output raw JSON only. Do not use Markdown blocks."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,
        "response_format": {"type": "json_object"}
    }
    
    try:
        response = requests.post(API_URL, json=payload, headers={
            "Authorization": f"Bearer {API_KEY}",
            "Content-Type": "application/json"
        }, timeout=120)
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            # æ¸…æ´— Markdown
            content = content.replace('```json', '').replace('```', '').strip()
            
            try:
                llm_json = json.loads(content)
                
                # æ•°æ®å›å¡«
                final_topics = []
                for topic_obj in llm_json.get('top_topics', []):
                    enriched_tweets = []
                    for t_ref in topic_obj.get('tweet_ids', []):
                        tid = t_ref.get('id')
                        stance = t_ref.get('stance', 'neutral')
                        trans = t_ref.get('translation', 'æš‚æ— ç¿»è¯‘') # è·å–ç¿»è¯‘
                        
                        if isinstance(tid, int) and 0 <= tid < len(top_items):
                            original = top_items[tid]
                            enriched_tweets.append({
                                "text": original.get('full_text', ''),
                                "translation": trans, # å­˜å…¥ç¿»è¯‘
                                "stance": stance,
                                "username": original.get('username', 'Unknown'),
                                "created_at": original.get('created_at', ''),
                                "metrics": {
                                    "reply": original.get('reply_count', 0),
                                    "retweet": original.get('retweet_count', 0),
                                    "like": original.get('favorite_count', 0)
                                }
                            })
                    
                    if enriched_tweets:
                        final_topics.append({
                            "topic": topic_obj.get('topic'),
                            "tweets": enriched_tweets
                        })
                
                return {
                    "top_topics": final_topics,
                    "hot_words": llm_json.get('hot_words', [])
                }
            except json.JSONDecodeError:
                print(f"âŒ JSON è§£æå¤±è´¥ [{region}]")
                return None
    except Exception as e:
        print(f"âš ï¸ Error ({region}): {e}")
    
    return None

def main():
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œå•æ—¥è¯é¢˜åˆ†ææ¨¡å¼ | ç›®æ ‡æ—¥æœŸ: {TARGET_DATE}")
    
    out_path = os.path.join(OUTPUT_DIR, f"{TARGET_DATE}.json")
    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)
    
    regions_data = load_data_for_target_date(TARGET_DATE)
    
    if not regions_data:
        print(f"âš ï¸ æœªæ‰¾åˆ°æ—¥æœŸ {TARGET_DATE} çš„ä»»ä½•æ•°æ®ã€‚")
        return

    print(f"\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"ğŸ”„ æ­£åœ¨åˆ†æ: {TARGET_DATE}")
    
    daily_result = {}
    current_fingerprint = get_files_fingerprint(TARGET_DATE)
    
    for region, items in regions_data.items():
        print(f"   -> æ­£åœ¨å¤„ç† [{region}] æ¿å—...")
        analysis = call_llm_analysis(region, TARGET_DATE, items)
        
        if analysis:
            daily_result[region] = {
                "region": region,
                "time_range": [TARGET_DATE, TARGET_DATE],
                "top_topics": analysis.get('top_topics', []),
                "hot_words": analysis.get('hot_words', [])
            }
        else:
            daily_result[region] = {"top_topics": [], "hot_words": []}
    
    daily_result["_meta"] = {
        "fingerprint": current_fingerprint,
        "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(daily_result, f, ensure_ascii=False, indent=2)
    print(f"âœ… ç”ŸæˆæˆåŠŸ: {out_path}")

if __name__ == "__main__":
    main()