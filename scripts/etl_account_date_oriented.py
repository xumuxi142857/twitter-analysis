import json
import os
import re
import requests
import hashlib
from datetime import datetime

# ================= é…ç½®åŒºåŸŸ =================
API_KEY = "sk-mwphmyljrynungesqkaqnbimwghczzpniulmdgepgswhjrco" 
API_URL = "https://api.siliconflow.cn/v1/chat/completions"
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_DIR = os.path.join(BASE_DIR, 'database', 'raw')
# è¾“å‡ºç›®å½•: public/db/account
OUTPUT_DIR = os.path.join(BASE_DIR, 'public', 'db', 'account')

FILENAME_MAPPING = {
    "Taiwan": "Taiwan",
    "US": "US",
    "Philippines": "Philippines",
    "Japan": "Japan",
    "JP": "Japan",
    "ph": "Philippines",
    "us": "US",
    "jp": "Japan",
    "tw": "Taiwan"
}
# ===========================================

def parse_date_from_filename(filename):
    match = re.search(r'(\d{8})_(\d{6})', filename)
    if match:
        date_str = match.group(1)
        return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
    return None

def get_files_fingerprint(date_key):
    """
    è®¡ç®—æŸä¸€æ—¥æœŸä¸‹æ‰€æœ‰æºæ–‡ä»¶çš„â€œæŒ‡çº¹â€ã€‚
    åªè¦æ–‡ä»¶åˆ—è¡¨å˜äº†ã€æ–‡ä»¶å¤§å°å˜äº†ã€æˆ–è€…ä¿®æ”¹æ—¶é—´å˜äº†ï¼ŒæŒ‡çº¹å°±ä¼šå˜ã€‚
    """
    target_date_str = date_key.replace("-", "") # 2025-12-25 -> 20251225
    related_files = []
    
    if os.path.exists(RAW_DIR):
        for f in os.listdir(RAW_DIR):
            # åªè¦æ–‡ä»¶ååŒ…å«è¯¥æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå°±è®¤ä¸ºæ˜¯è¯¥æ—¥æœŸçš„æºæ–‡ä»¶
            if target_date_str in f and f.endswith('.json'):
                path = os.path.join(RAW_DIR, f)
                # è®°å½•æ–‡ä»¶åã€å¤§å°ã€ä¿®æ”¹æ—¶é—´
                stat = os.stat(path)
                related_files.append(f"{f}_{stat.st_size}_{stat.st_mtime}")
    
    if not related_files:
        return None

    # æ’åºå¹¶æ‹¼æ¥ (ä¿è¯é¡ºåºä¸€è‡´æ€§)
    related_files.sort()
    combined_str = "|".join(related_files)
    
    # ç”Ÿæˆ MD5 å“ˆå¸Œ
    return hashlib.md5(combined_str.encode('utf-8')).hexdigest()

def check_needs_update(output_file, current_fingerprint):
    """
    å¯¹æ¯”æŒ‡çº¹æ¥å†³å®šæ˜¯å¦æ›´æ–°
    """
    # 1. å¦‚æœè¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¿…é¡»æ›´æ–°
    if not os.path.exists(output_file):
        return True
    
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # è·å–ä¸Šæ¬¡ä¿å­˜çš„æŒ‡çº¹ (åœ¨ _meta å­—æ®µé‡Œ)
            saved_fingerprint = data.get('_meta', {}).get('fingerprint', '')
            
            # 2. å¦‚æœæŒ‡çº¹ä¸ä¸€æ ·ï¼Œè¯´æ˜æºæ–‡ä»¶æœ‰å˜åŠ¨ï¼Œéœ€è¦æ›´æ–°
            if saved_fingerprint != current_fingerprint:
                return True
            
            # 3. æŒ‡çº¹ä¸€æ ·ï¼Œæ— éœ€æ›´æ–°
            return False
    except:
        # è¯»å–å‡ºé”™åˆ™å¼ºåˆ¶æ›´æ–°
        return True

def load_and_group_by_date_user():
    """
    åŠ è½½æ•°æ®ï¼Œè¿”å›ç»“æ„:
    {
        "2025-12-25": {
            "Philippines": { "user1": [tweets...], "user2": [tweets...] },
            "US": ...
        }
    }
    """
    grouped_data = {}
    
    if not os.path.exists(RAW_DIR): return grouped_data

    for filename in os.listdir(RAW_DIR):
        if not filename.endswith('.json'): continue
        
        date_key = parse_date_from_filename(filename)
        if not date_key: continue
            
        target_region = None
        for key, region in FILENAME_MAPPING.items():
            if key.lower() in filename.lower():
                target_region = region
                break
        if not target_region: continue

        # åˆå§‹åŒ–å±‚çº§
        if date_key not in grouped_data: grouped_data[date_key] = {}
        if target_region not in grouped_data[date_key]: grouped_data[date_key][target_region] = {}

        path = os.path.join(RAW_DIR, filename)
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                items = data if isinstance(data, list) else [data]
                
                for item in items:
                    uname = item.get('username', 'Unknown')
                    text = item.get('full_text', '')
                    if uname not in grouped_data[date_key][target_region]:
                        grouped_data[date_key][target_region][uname] = []
                    grouped_data[date_key][target_region][uname].append(text)
        except: pass

    return grouped_data

def analyze_user_profile(username, tweets):
    """è°ƒç”¨ LLM åˆ†æå•ä¸ªç”¨æˆ·ç”»åƒ"""
    content_str = "\n---\n".join(tweets[:15]) # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªç¤¾ä¼šå¿ƒç†å­¦ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ· "{username}" çš„æ¨æ–‡ç”Ÿæˆç”»åƒã€‚
    
    æ¨æ–‡è®°å½•ï¼š
    {content_str}

    è¯·è¿”å›ä¸¥æ ¼ JSONï¼š
    1. info: ä¸€å¥è¯æ¦‚æ‹¬äººè®¾(50å­—å†…)ã€‚
    2. stance_matrix: å¯¹ä¸­ç«‹åœºçŸ©é˜µ [[x(ç«‹åœº0-2), y(ç»´åº¦0-3), value(0-10)]...]ã€‚ç»´åº¦:0æ”¿1å†›2ç»3æ–‡; ç«‹åœº:0è´Ÿ1ä¸­2æ­£ã€‚
    3. influence_type: äº²æƒ…/åŒä¼´/æƒå¨ ä¸‰ç±»å æ¯”ã€‚

    JSON ç¤ºä¾‹ï¼š
    {{
        "info": "æ¿€è¿›çš„å†›äº‹è¯„è®ºå‘˜...",
        "stance_matrix": [[0,0,8], [1,0,2]...], 
        "influence_type": [{{"name": "æƒå¨ (Authority)", "value": 80}}, {{"name": "åŒä¼´ (Peer)", "value": 20}}]
    }}
    """

    try:
        response = requests.post(API_URL, json={
            "model": MODEL_NAME,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "response_format": {"type": "json_object"}
        }, headers={"Authorization": f"Bearer {API_KEY}"})
        
        if response.status_code == 200:
            return json.loads(response.json()['choices'][0]['message']['content'])
    except: pass
    return None

def main():
    print("ğŸš€ å¼€å§‹æŒ‰æ—¥æœŸå¤„ç†è´¦å·æ¨èæ•°æ® (æ™ºèƒ½å¢é‡æ›´æ–°ç‰ˆ - Account)...")
    date_groups = load_and_group_by_date_user()
    
    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)

    for date_key, regions_data in date_groups.items():
        # å®šä¹‰è¾“å‡ºæ–‡ä»¶è·¯å¾„
        out_path = os.path.join(OUTPUT_DIR, f"{date_key}.json")
        
        # --- æ™ºèƒ½æ›´æ–°åˆ¤æ–­æ ¸å¿ƒ ---
        current_fingerprint = get_files_fingerprint(date_key)
        
        if not check_needs_update(out_path, current_fingerprint):
            print(f"â© æ—¥æœŸ {date_key} æºæ–‡ä»¶é›†æœªå˜åŠ¨ï¼Œè·³è¿‡ (å·²èŠ‚çœ Token)")
            continue
        # ----------------------

        print(f"\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"ğŸ”„ æ£€æµ‹åˆ°æ•°æ®å˜åŠ¨ï¼Œæ­£åœ¨å¤„ç†è´¦å·ç”»åƒ: {date_key}")
        
        daily_result = {} 
        
        for region, users_map in regions_data.items():
            print(f"   -> æ¿å— [{region}] å…±æœ‰ {len(users_map)} ä¸ªæ´»è·ƒç”¨æˆ·")
            
            # 1. ç®€å•æ’åºï¼šå–å‘å¸–é‡æœ€å¤šçš„ Top 5 (èŠ‚çœ Token)
            sorted_users = sorted(users_map.items(), key=lambda x: len(x[1]), reverse=True)[:5]
            
            analyzed_list = []
            for uname, tweets in sorted_users:
                print(f"      åˆ†æç”¨æˆ·: {uname}...")
                profile = analyze_user_profile(uname, tweets)
                if profile:
                    profile['username'] = uname
                    profile['tweet_count'] = len(tweets)
                    analyzed_list.append(profile)
            
            daily_result[region] = {
                "region": region,
                "time_range": [date_key, date_key],
                "top_users": analyzed_list
            }
        
        # å†™å…¥æ–‡ä»¶ï¼ŒåŒæ—¶å†™å…¥ _meta æŒ‡çº¹ä¿¡æ¯
        daily_result["_meta"] = {
            "fingerprint": current_fingerprint,
            "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
            
        try:
            with open(out_path, 'w', encoding='utf-8') as f:
                json.dump(daily_result, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ›´æ–°æˆåŠŸ: {out_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")

    print("\nğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()