import json
import os
import re
import requests
import hashlib
from datetime import datetime

# ================= é…ç½®åŒºåŸŸ =================
# ğŸ“… æŒ‡å®šæ—¥æœŸ
TARGET_DATE = "2026-01-21"

API_KEY = "sk-mwphmyljrynungesqkaqnbimwghczzpniulmdgepgswhjrco" 
API_URL = "https://api.siliconflow.cn/v1/chat/completions"
MODEL_NAME = "Pro/zai-org/GLM-4.7" 

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_DIR = os.path.join(BASE_DIR, 'database', 'raw')
OUTPUT_DIR = os.path.join(BASE_DIR, 'public', 'db', 'account1')

FILENAME_MAPPING = {
    "Taiwan": "Taiwan",
    "US": "US",
    "Philippines": "Philippines",
    "Japan": "Japan",
    "JP": "Japan",
    "ph": "Philippines",
    "us": "US",
    "jp": "Japan",
    "tw": "Taiwan"
}
# ===========================================

def get_files_fingerprint(date_key):
    """è®¡ç®—ç›®æ ‡æ—¥æœŸä¸‹ç›¸å…³æ–‡ä»¶çš„æŒ‡çº¹"""
    target_date_str = date_key.replace("-", "")
    related_files = []
    if os.path.exists(RAW_DIR):
        for root, dirs, files in os.walk(RAW_DIR):
            for f in files:
                if target_date_str in f and f.endswith('.json'):
                    path = os.path.join(root, f)
                    stat = os.stat(path)
                    related_files.append(f"{f}_{stat.st_size}_{stat.st_mtime}")
    if not related_files: return None
    related_files.sort()
    combined_str = "|".join(related_files)
    return hashlib.md5(combined_str.encode('utf-8')).hexdigest()

def check_needs_update(output_file, current_fingerprint):
    if not os.path.exists(output_file): return True
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            saved_fingerprint = data.get('_meta', {}).get('fingerprint', '')
            return saved_fingerprint != current_fingerprint
    except: return True

def load_data_for_target_date(target_date):
    """åªåŠ è½½æŒ‡å®šæ—¥æœŸçš„æ•°æ®ï¼ŒæŒ‰ç”¨æˆ·åˆ†ç»„"""
    grouped_data = {}
    target_date_str = target_date.replace("-", "")
    
    if not os.path.exists(RAW_DIR): return grouped_data

    print(f"ğŸ“‚ æ­£åœ¨æ‰«æ {RAW_DIR} ä¸­åŒ…å« '{target_date_str}' çš„æ–‡ä»¶...")
    file_count = 0
    
    for root, dirs, files in os.walk(RAW_DIR):
        for filename in files:
            if not filename.endswith('.json'): continue
            if target_date_str not in filename: continue
            
            target_region = None
            for key, region in FILENAME_MAPPING.items():
                if key.lower() in filename.lower():
                    target_region = region
                    break
            if not target_region: continue

            if target_region not in grouped_data: grouped_data[target_region] = {}

            path = os.path.join(root, filename)
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    items = data if isinstance(data, list) else [data]
                    
                    for item in items:
                        uname = item.get('username', 'Unknown')
                        if item.get('full_text'):
                            if uname not in grouped_data[target_region]:
                                grouped_data[target_region][uname] = []
                            grouped_data[target_region][uname].append(item)
                    file_count += 1
            except: pass

    print(f"âœ… æ‰«æå®Œæˆï¼Œå…±æ‰¾åˆ° {file_count} ä¸ªç›¸å…³æ–‡ä»¶")
    return grouped_data

def analyze_user_profile(username, raw_tweets):
    """
    LLM åˆ†æï¼šç”¨æˆ·ç”»åƒ + æ¨æ–‡ç¿»è¯‘ä¸ç«‹åœº
    """
    if not raw_tweets: return None

    # 1. æ™ºèƒ½é‡‡æ ·ï¼šTop 15
    def calculate_impact(item):
        return (item.get('retweet_count', 0)*2) + item.get('reply_count', 0) + (item.get('favorite_count', 0)*0.5)
    
    sorted_tweets = sorted(raw_tweets, key=calculate_impact, reverse=True)
    top_tweets = sorted_tweets[:10]
    
    # 2. æ„å»ºè¾“å…¥
    input_list = []
    for idx, t in enumerate(top_tweets):
        text = t.get('full_text', '').replace('\n', ' ').strip()
        if len(text) > 5:
            input_list.append(f"ID[{idx}]: {text}")
    
    input_text_str = "\n".join(input_list)
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæƒ…æŠ¥åˆ†æä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ· "{username}" çš„æ¨æ–‡è®°å½•è¿›è¡Œç”»åƒã€‚
    
    æ¨æ–‡åˆ—è¡¨:
    {input_text_str}

    ä»»åŠ¡ï¼š
    1. ã€ç”»åƒç”Ÿæˆã€‘
       - info: æå…¶ç²¾ç®€çš„æƒ…æŠ¥ç®€è¿°ï¼Œæ§åˆ¶åœ¨20å­—ä»¥å†…ï¼Œä¸è¦æ¢è¡Œã€‚
       - stance_matrix: ç«‹åœºçƒ­åŠ›å›¾æ•°æ®ï¼Œæ ¼å¼ä¸º [[x, y, value], ...] çš„äºŒç»´æ•°ç»„ã€‚
         **åæ ‡å®šä¹‰ä¸¥æ ¼éµå®ˆä»¥ä¸‹æ ‡å‡†ï¼Œä¸è¦æé”™ï¼š**
         * xè½´ (ç«‹åœº): 0=åå(Negative), 1=ä¸­ç«‹(Neutral), 2=äº²å(Positive)
         * yè½´ (é¢†åŸŸ): 0=æ”¿æ²»(Political), 1=å†›äº‹(Military), 2=ç»æµ(Economic), 3=æ–‡åŒ–(Cultural)
         * value (å¼ºåº¦): 0-10 çš„æ•´æ•°
       - influence_type: äº²æƒ…/åŒä¼´/æƒå¨ ä¸‰ç±»å æ¯”ã€‚
    
    2. ã€æ¨æ–‡ç ”åˆ¤ã€‘
       - å¯¹æ¯ä¸€æ¡æ¨æ–‡è¿›è¡Œé’ˆå¯¹ä¸­å›½å¤§é™†çš„ç«‹åœºåˆ¤æ–­ï¼ˆå¦‚æœæ˜¯åååˆ™ä¸ºnegativeï¼‰ (positive/neutral/negative)ã€‚
       - **å¿…é¡»**æä¾›è¯¥æ¨æ–‡çš„ä¸­æ–‡ç¿»è¯‘ (translation)ã€‚
       - å®‰å…¨å®¡æŸ¥ï¼šå¦‚æœç”¨æˆ·å‘å¸ƒè‰²æƒ…å†…å®¹ï¼Œæˆ–è€…æ•°æ®æ— æ³•åˆ†æï¼Œè¯·åŠ¡å¿…å°† info å­—æ®µè®¾ç½®ä¸ºå­—ç¬¦ä¸² "INVALID_USER"ï¼Œä¸è¦è¾“å‡ºå…¶ä»–è§£é‡Šã€‚
    
    è¾“å‡º JSON æ ¼å¼ï¼ˆä¸¥ç¦Markdownï¼‰ï¼š
    {{
        "info": "ååæ¿€è¿›æ´¾ï¼Œä¸»è¦å…³æ³¨å—æµ·å†›äº‹è®®é¢˜ã€‚",
        "stance_matrix": [[0,1,9]...], 
        "influence_type": [{{"name": "æƒå¨", "value": 80}}...],
        "tweet_analysis": [
            {{"id": 0, "stance": "negative", "translation": "è¿™é‡Œæ˜¯æ¨æ–‡0çš„ä¸­æ–‡ç¿»è¯‘..."}},
            {{"id": 1, "stance": "neutral", "translation": "è¿™é‡Œæ˜¯æ¨æ–‡1çš„ä¸­æ–‡ç¿»è¯‘..."}}
        ]
    }}
    """

    try:
        response = requests.post(API_URL, json={
            "model": MODEL_NAME,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "response_format": {"type": "json_object"}
        }, headers={"Authorization": f"Bearer {API_KEY}"}, timeout=60)
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            content = content.replace('```json', '').replace('```', '').strip()
            res_json = json.loads(content)
            
            info_text = res_json.get("info", "")
            
            # 1. æ£€æŸ¥ Prompt çº¦å®šçš„ç‰¹å®šæ ‡è¯†
            if "INVALID_USER" in info_text:
                print(f"ğŸ›‘ æ‹¦æˆªæ— æ•ˆç”¨æˆ· [{username}]: LLM åˆ¤å®šä¸ºæ— æ•ˆ/è¿è§„")
                return None
            
            # 2. å…³é”®è¯å…œåº•ï¼ˆé˜²æ­¢ LLM ä¸å¬è¯ï¼Œæ²¡è¾“å‡º INVALID_USER è€Œæ˜¯è¾“å‡ºäº†äººè¯ï¼‰
            block_keywords = ["è‰²æƒ…", "æ— æ³•ç”Ÿæˆ", "ç§»é™¤", "adult", "porn","æ•°æ®å¼‚å¸¸","å¼‚å¸¸"]
            if any(k in info_text for k in block_keywords):
                 print(f"ğŸ›‘ æ‹¦æˆªæ•æ„Ÿç”¨æˆ· [{username}]: è§¦å‘å…³é”®è¯è¿‡æ»¤")
                 return None
                 
            # 3. æ£€æŸ¥çŸ©é˜µæ•°æ®æ˜¯å¦ä¸ºç©º
            if not res_json.get("stance_matrix"):
                print(f"âš ï¸ æ‹¦æˆªç©ºæ•°æ®ç”¨æˆ· [{username}]: çŸ©é˜µæ•°æ®ç¼ºå¤±")
                return None
            # =======================================================
            
            # 3. æ•°æ®å›å¡« (åŒ…å«ç¿»è¯‘)
            enriched_tweets = []
            analysis_map = {item['id']: item for item in res_json.get('tweet_analysis', [])}
            
            for idx, tweet in enumerate(top_tweets):
                analysis = analysis_map.get(idx, {})
                stance = analysis.get('stance', 'neutral')
                trans = analysis.get('translation', 'æš‚æ— ç¿»è¯‘') 
                
                enriched_tweets.append({
                    "text": tweet.get('full_text', ''),
                    "translation": trans,
                    "stance": stance,
                    "username": tweet.get('username', username),
                    "created_at": tweet.get('created_at', ''),
                    "metrics": {
                        "reply": tweet.get('reply_count', 0),
                        "retweet": tweet.get('retweet_count', 0),
                        "like": tweet.get('favorite_count', 0)
                    }
                })
            
            return {
                "info": info_text,
                "stance_matrix": res_json.get("stance_matrix"),
                "influence_type": res_json.get("influence_type"),
                "tweets": enriched_tweets
            }

    except Exception as e:
        print(f"âš ï¸ Error analyzing {username}: {e}")
    return None

def main():
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œå•æ—¥è´¦å·ç”»åƒåˆ†æ | ç›®æ ‡æ—¥æœŸ: {TARGET_DATE}")
    
    out_path = os.path.join(OUTPUT_DIR, f"{TARGET_DATE}.json")
    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)
    
    regions_data = load_data_for_target_date(TARGET_DATE)
    
    if not regions_data:
        print(f"âš ï¸ æœªæ‰¾åˆ°æ—¥æœŸ {TARGET_DATE} çš„æ•°æ®ã€‚")
        return

    print(f"\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"ğŸ”„ æ­£åœ¨åˆ†æ: {TARGET_DATE}")
    
    daily_result = {}
    current_fingerprint = get_files_fingerprint(TARGET_DATE)
    
    # æŒ‡çº¹æ£€æŸ¥ (å¯æ³¨é‡Šæ‰å¼ºåˆ¶è¿è¡Œ)
    if not check_needs_update(out_path, current_fingerprint):
         print(f"â© æ—¥æœŸ {TARGET_DATE} æ•°æ®æœªå˜åŠ¨ï¼Œè·³è¿‡å¤„ç†")
         return

    for region, users_map in regions_data.items():
        print(f"   -> æ¿å— [{region}] å…±æœ‰ {len(users_map)} ä¸ªæ´»è·ƒç”¨æˆ·")
        
        # ã€ä¿®æ”¹ã€‘å– Top 10
        sorted_users = sorted(users_map.items(), key=lambda x: len(x[1]), reverse=True)[:10]
        
        analyzed_list = []
        for uname, tweets in sorted_users:
            print(f"      æ­£åœ¨ç”»åƒ: {uname}...")
            profile = analyze_user_profile(uname, tweets)
            
            if profile:
                profile['username'] = uname
                profile['tweet_count'] = len(tweets)
                analyzed_list.append(profile)
        
        daily_result[region] = {
            "region": region,
            "time_range": [TARGET_DATE, TARGET_DATE],
            "top_users": analyzed_list
        }
    
    daily_result["_meta"] = {
        "fingerprint": current_fingerprint,
        "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
        
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(daily_result, f, ensure_ascii=False, indent=2)
    print(f"âœ… ç”ŸæˆæˆåŠŸ: {out_path}")

if __name__ == "__main__":
    main()