import json
import os
import re
import requests
import hashlib
import traceback
from datetime import datetime

# ================= é…ç½®åŒºåŸŸ =================
# ğŸ“… æŒ‡å®šæ—¥æœŸ
TARGET_DATE = "2025-12-25" 

API_KEY = "sk-mwphmyljrynungesqkaqnbimwghczzpniulmdgepgswhjrco"
API_URL = "https://api.siliconflow.cn/v1/chat/completions"
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_DIR = os.path.join(BASE_DIR, 'database', 'raw')
OUTPUT_DIR = os.path.join(BASE_DIR, 'public', 'db', 'topic')

FILENAME_MAPPING = {
    "Taiwan": "Taiwan",
    "China_US": "US", 
    "Philippines": "Philippines",
    "Japan": "Japan",
    "JP": "Japan",
    "ph": "Philippines",
    "us": "US",
    "jp": "Japan",
    "tw": "Taiwan"
}
# ===========================================

def parse_date_from_filename(filename):
    # å…¼å®¹å¸¦æ—¶é—´åç¼€å’Œä¸å¸¦åç¼€çš„æ–‡ä»¶å
    match = re.search(r'(20[2-3]\d{5})', filename)
    if match:
        date_str = match.group(1)
        return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
    return None

def get_files_fingerprint(date_key):
    target_date_str = date_key.replace("-", "") 
    related_files = []
    
    if os.path.exists(RAW_DIR):
        for root, dirs, files in os.walk(RAW_DIR):
            for f in files:
                if target_date_str in f and f.endswith('.json'):
                    path = os.path.join(root, f)
                    stat = os.stat(path)
                    related_files.append(f"{f}_{stat.st_size}_{stat.st_mtime}")
    
    if not related_files: return None
    related_files.sort()
    combined_str = "|".join(related_files)
    return hashlib.md5(combined_str.encode('utf-8')).hexdigest()

def load_data_for_target_date(target_date):
    region_data = {}
    target_date_str = target_date.replace("-", "")
    
    if not os.path.exists(RAW_DIR):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°ç›®å½• {RAW_DIR}")
        return region_data

    print(f"ğŸ“‚ æ­£åœ¨æ‰«æ {RAW_DIR} ä¸­åŒ…å« '{target_date_str}' çš„æ–‡ä»¶...")
    file_count = 0
    
    for root, dirs, files in os.walk(RAW_DIR):
        for filename in files:
            if not filename.endswith('.json'): continue
            if target_date_str not in filename: continue
            
            target_region = None
            for key, region in FILENAME_MAPPING.items():
                if key.lower() in filename.lower(): 
                    target_region = region
                    break
            if not target_region: continue

            if target_region not in region_data: region_data[target_region] = []

            path = os.path.join(root, filename)
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    items = data if isinstance(data, list) else [data]
                    for item in items:
                        if item.get('full_text'):
                            region_data[target_region].append(item)
                    file_count += 1
            except: pass
            
    print(f"âœ… æ‰«æå®Œæˆï¼Œå…±æ‰¾åˆ° {file_count} ä¸ªç›¸å…³æ–‡ä»¶")
    return region_data

def call_llm_analysis(region, date, raw_items):
    """
    ã€å¢å¼ºç‰ˆã€‘æ™ºèƒ½é‡‡æ · + LLM åˆ†æ + é”™è¯¯è°ƒè¯•
    """
    if not raw_items: return None

    # 1. å»é‡
    unique_items = {}
    for item in raw_items:
        key = item.get('tweet_id') or item.get('full_text')
        unique_items[key] = item
    clean_items = list(unique_items.values())

    # 2. æŒ‰å½±å“åŠ›æ’åº
    def calculate_impact(item):
        retweet = item.get('retweet_count', 0) or 0
        reply = item.get('reply_count', 0) or 0
        like = item.get('favorite_count', 0) or 0
        return (retweet * 2) + (reply * 1) + (like * 0.5)

    clean_items.sort(key=calculate_impact, reverse=True)

    # 3. æˆªå– Top 50
    top_items = clean_items[:50]
    print(f"      [é‡‡æ ·] {region}: åŸå§‹ {len(raw_items)} æ¡ -> ç²¾é€‰ Top {len(top_items)} æ¡")

    # 4. æ„å»º Prompt
    input_list = []
    for idx, item in enumerate(top_items):
        text = item.get('full_text', '').replace('\n', ' ').strip()
        if len(text) > 15:
            input_list.append(f"ID[{idx}]: {text}")
    
    input_text_str = "\n".join(input_list)
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæƒ…æŠ¥åˆ†æå‘˜ã€‚è¯·åˆ†æä»¥ä¸‹â€œ{region}â€æ¿å—çš„æ¨ç‰¹æ–‡æœ¬ã€‚
    
    æ–‡æœ¬åˆ—è¡¨ (å¸¦ID):
    {input_text_str}

    ä»»åŠ¡ï¼š
    1. ã€è¯é¢˜èšç±»ã€‘å°†æ¨æ–‡èšç±»ä¸º Top 10 æ ¸å¿ƒè¯é¢˜ã€‚ä¸€å®šä¸è¦æœ‰é‡å¤çš„è¯é¢˜ã€‚
    2. ã€ç«‹åœºç ”åˆ¤ã€‘åˆ—å‡ºæ¯ä¸ªè¯é¢˜ä¸‹çš„æ¨æ–‡IDï¼Œå¹¶åˆ¤æ–­è¯¥æ¨æ–‡çš„ç«‹åœº(positive/neutral/negative)ã€‚
    3. ã€è¯äº‘æå–ã€‘æå– Top 15 çƒ­é—¨å…³é”®è¯ ,è¦ç¿»è¯‘ä¸ºä¸­æ–‡(æ’é™¤é€šç”¨å›½å®¶åï¼Œåªä¿ç•™å…·ä½“äº‹ä»¶/å®ä½“)ã€‚
    
    è¾“å‡º JSON æ ¼å¼ï¼š
    {{
        "top_topics": [
            {{
                "topic": "è¯é¢˜æ‘˜è¦",
                "tweet_ids": [
                    {{"id": 0, "stance": "negative"}},
                    {{"id": 3, "stance": "neutral"}}
                ]
            }}
        ],
        "hot_words": [
            {{"name": "å…·ä½“åè¯", "value": 88}}
        ]
    }}
    """

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": "You are a data analyst. Output raw JSON only. Do not use Markdown blocks."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,
        "response_format": {"type": "json_object"}
    }
    
    try:
        response = requests.post(API_URL, json=payload, headers={
            "Authorization": f"Bearer {API_KEY}",
            "Content-Type": "application/json"
        }, timeout=60) # å¢åŠ è¶…æ—¶è®¾ç½®
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            
            # ã€å…³é”®ä¿®æ”¹ã€‘æ¸…æ´— Markdown æ ‡è®°ï¼Œé˜²æ­¢ JSON è§£ææŒ‚æ‰
            content = content.replace('```json', '').replace('```', '').strip()
            
            try:
                llm_json = json.loads(content)
            except json.JSONDecodeError:
                print(f"âŒ JSON è§£æå¤±è´¥ [{region}]ï¼LLM è¿”å›äº†é JSON å†…å®¹ã€‚")
                print(f"ğŸ” è¿”å›å†…å®¹ç‰‡æ®µ: {content[:100]}...")
                return None
            
            # æ•°æ®å›å¡«
            final_topics = []
            for topic_obj in llm_json.get('top_topics', []):
                enriched_tweets = []
                for t_ref in topic_obj.get('tweet_ids', []):
                    tid = t_ref.get('id')
                    stance = t_ref.get('stance', 'neutral')
                    
                    if isinstance(tid, int) and 0 <= tid < len(top_items):
                        original = top_items[tid]
                        enriched_tweets.append({
                            "text": original.get('full_text', ''),
                            "stance": stance,
                            "username": original.get('username', 'Unknown'),
                            "created_at": original.get('created_at', ''),
                            "metrics": {
                                "reply": original.get('reply_count', 0),
                                "retweet": original.get('retweet_count', 0),
                                "like": original.get('favorite_count', 0)
                            }
                        })
                
                if enriched_tweets:
                    final_topics.append({
                        "topic": topic_obj.get('topic'),
                        "tweets": enriched_tweets
                    })
            
            return {
                "top_topics": final_topics,
                "hot_words": llm_json.get('hot_words', [])
            }
        else:
            print(f"âŒ API è¯·æ±‚å¤±è´¥ [{region}]: {response.status_code}")
            print(f"ğŸ” é”™è¯¯è¯¦æƒ…: {response.text}")
            
    except Exception as e:
        print(f"âŒ å¤„ç†å¼‚å¸¸ [{region}]: {e}")
        traceback.print_exc() # æ‰“å°å®Œæ•´æŠ¥é”™å †æ ˆ
    
    return None

def main():
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œå•æ—¥è¯é¢˜åˆ†ææ¨¡å¼ | ç›®æ ‡æ—¥æœŸ: {TARGET_DATE}")
    
    out_path = os.path.join(OUTPUT_DIR, f"{TARGET_DATE}.json")
    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)
    
    regions_data = load_data_for_target_date(TARGET_DATE)
    
    if not regions_data:
        print(f"âš ï¸ æœªæ‰¾åˆ°æ—¥æœŸ {TARGET_DATE} çš„ä»»ä½•æ•°æ®ã€‚")
        return

    print(f"\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"ğŸ”„ æ­£åœ¨åˆ†æ: {TARGET_DATE}")
    
    # å¼ºåˆ¶é‡æ–°åˆ†æï¼Œä¸ºäº†è°ƒè¯•ï¼Œæš‚æ—¶æ³¨é‡Šæ‰æŒ‡çº¹è·³è¿‡é€»è¾‘
    # å¦‚æœæƒ³æ¢å¤è·³è¿‡ï¼Œè¯·å–æ¶ˆä¸‹é¢ä¸¤è¡Œçš„æ³¨é‡Š
    # if not check_needs_update(out_path, get_files_fingerprint(TARGET_DATE)):
    #     print(f"â© æ•°æ®æœªå˜åŠ¨ï¼Œè·³è¿‡")
    #     return
    
    daily_result = {}
    current_fingerprint = get_files_fingerprint(TARGET_DATE)
    
    for region, items in regions_data.items():
        print(f"   -> æ­£åœ¨å¤„ç† [{region}] æ¿å—...")
        
        analysis = call_llm_analysis(region, TARGET_DATE, items)
        
        if analysis:
            print(f"      âœ… [{region}] åˆ†ææˆåŠŸ")
            daily_result[region] = {
                "region": region,
                "time_range": [TARGET_DATE, TARGET_DATE],
                "top_topics": analysis.get('top_topics', []),
                "hot_words": analysis.get('hot_words', [])
            }
        else:
            print(f"      âš ï¸ [{region}] åˆ†æè¿”å›ä¸ºç©ºï¼Œç»“æœå°†ä¸ºç©ºç™½")
            daily_result[region] = {"top_topics": [], "hot_words": []}
    
    daily_result["_meta"] = {
        "fingerprint": current_fingerprint,
        "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(daily_result, f, ensure_ascii=False, indent=2)
    print(f"âœ… ç”ŸæˆæˆåŠŸ: {out_path}")

if __name__ == "__main__":
    main()