import json
import os
import requests
import hashlib
from datetime import datetime
from dateutil import parser
import traceback

# ================= é…ç½®åŒºåŸŸ =================
# ğŸ¯ ç›®æ ‡åç§°
TARGET_NAME = "asahi" 

API_KEY = "sk-7ba052d40efe48ae990141e577d952d1"  # 
API_URL = "https://api.deepseek.com/chat/completions"
MODEL_NAME = "deepseek-chat"  # 

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
PROFILE_DIR = os.path.join(BASE_DIR, 'database', 'raw', 'profile')
CONFIG_FILE = os.path.join(PROFILE_DIR, 'targets.json')

# è¾“å‡ºç›®å½•
DETECT_DB_DIR = os.path.join(BASE_DIR, 'public', 'db', 'detect')
LIST_FILE = os.path.join(DETECT_DB_DIR, 'list.json')
DETAILS_DIR = os.path.join(DETECT_DB_DIR, 'details')
# ===========================================

def get_file_fingerprint(file_path):
    if not os.path.exists(file_path): return None
    stat = os.stat(file_path)
    identifier = f"{os.path.basename(file_path)}_{stat.st_size}_{stat.st_mtime}"
    return hashlib.md5(identifier.encode('utf-8')).hexdigest()

def calculate_stats(tweets):
    if not tweets: return 0
    dates = []
    for t in tweets:
        try:
            dt = parser.parse(t.get('created_at', ''))
            dates.append(dt)
        except: continue
    if not dates: return 0
    delta_days = (max(dates) - min(dates)).days
    if delta_days < 1: delta_days = 1
    return round(len(tweets) / delta_days, 1)

def batch_analyze_tweets(tweets):
    """
    ã€æ–°åŠŸèƒ½ã€‘æ‰¹é‡åˆ†ææœ€æ–°çš„ 20 æ¡æ¨æ–‡ï¼šç¿»è¯‘ + åˆ¤ç«‹
    """
    if not tweets: return []
    
    # æ„å»ºè¾“å…¥åˆ—è¡¨
    input_text = ""
    for idx, t in enumerate(tweets):
        clean_text = t.get('full_text', '').replace('\n', ' ').strip()
        input_text += f"ID[{idx}]: {clean_text}\n"
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæƒ…æŠ¥ç¿»è¯‘å®˜ã€‚è¯·åˆ†æä»¥ä¸‹ç¤¾äº¤åª’ä½“æ¨æ–‡åˆ—è¡¨ã€‚
    
    è¾“å…¥å†…å®¹ï¼š
    {input_text}
    
    ä»»åŠ¡ï¼š
    1. ã€ä¸­æ–‡ç¿»è¯‘ã€‘ï¼šå°†æ¨æ–‡ç¿»è¯‘æˆæµç•…çš„ä¸­æ–‡ã€‚
    2. ã€å¯¹ä¸­ç«‹åœºã€‘ï¼šåˆ¤æ–­è¯¥æ¡æ¨æ–‡ä½“ç°çš„å¯¹åç«‹åœºï¼ˆè‹¥æ¨æ–‡ä¸ä¸­å›½æ— å…³ï¼Œæ ‡è®°ä¸ºâ€œæ— å…³â€ï¼‰ã€‚
       ç«‹åœºé€‰é¡¹ï¼šæ­£é¢ (Positive)ã€ä¸­ç«‹ (Neutral)ã€è´Ÿé¢ (Negative)ã€æ— å…³ (Irrelevant)ã€‚
    
    è¾“å‡ºè¦æ±‚ï¼š
    è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼Œé¡ºåºä¸è¾“å…¥ ID ä¸¥æ ¼å¯¹åº”ã€‚æ ¼å¼å¦‚ä¸‹ï¼š
    [
        {{ "id": 0, "trans": "ä¸­æ–‡ç¿»è¯‘å†…å®¹...", "stance": "è´Ÿé¢" }},
        {{ "id": 1, "trans": "ä¸­æ–‡ç¿»è¯‘...", "stance": "æ— å…³" }}
    ]
    """
    
    try:
        response = requests.post(API_URL, json={
            "model": MODEL_NAME,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "response_format": {"type": "json_object"}
        }, headers={"Authorization": f"Bearer {API_KEY}"}, timeout=120)
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            content = content.replace('```json', '').replace('```', '').strip()
            
            try:
                raw_json = json.loads(content)
                result_list = []
                if isinstance(raw_json, dict):
                    # å…¼å®¹ä¸åŒè¿”å›æ ¼å¼
                    for k, v in raw_json.items():
                        if isinstance(v, list): result_list = v
                elif isinstance(raw_json, list):
                    result_list = raw_json
                
                # å°†åˆ†æç»“æœåˆå¹¶å›åŸå§‹æ¨æ–‡
                enriched_tweets = []
                analysis_map = {item['id']: item for item in result_list}
                
                for idx, t in enumerate(tweets):
                    analysis = analysis_map.get(idx, {"trans": "ç¿»è¯‘å¤±è´¥", "stance": "ä¸­ç«‹"})
                    enriched_tweets.append({
                        "created_at": t.get('created_at'),
                        "text": t.get('full_text'),
                        "translation": analysis.get('trans'),
                        "stance": analysis.get('stance'),
                        "metrics": {
                            "reply": t.get('reply_count', 0),
                            "retweet": t.get('retweet_count', 0),
                            "like": t.get('favorite_count', 0)
                        }
                    })
                return enriched_tweets
            except:
                print("âŒ æ¨æ–‡æ‰¹é‡åˆ†æ JSON è§£æå¤±è´¥")
    except Exception as e:
        print(f"API Error (Batch Analysis): {e}")
    
    return [] # å¤±è´¥åˆ™è¿”å›ç©ºï¼Œæˆ–è€…è¿”å›æœªç¿»è¯‘çš„åŸå§‹æ•°æ®

def generate_deep_report(name, raw_tweets):
    """
    ç”Ÿæˆ 9 ç»´æŠ¥å‘Š + çŸ©é˜µ + é¥¼å›¾
    """
    # ... (ä¿æŒåŸæœ‰çš„é‡‡æ ·é€»è¾‘ä¸å˜ï¼Œä¸ºäº†èŠ‚çœç¯‡å¹…ï¼Œè¿™é‡Œå¤ç”¨ä¹‹å‰çš„é‡‡æ ·ä»£ç )
    def safe_parse_time(t):
        try: return parser.parse(t.get('created_at', ''))
        except: return datetime.min

    sorted_by_date = sorted(raw_tweets, key=safe_parse_time, reverse=True)
    recent_tweets = sorted_by_date[:20]
    
    def get_impact(t): return (t.get('retweet_count',0)*2 + t.get('reply_count',0))
    sorted_by_impact = sorted(raw_tweets, key=get_impact, reverse=True)
    top_tweets = sorted_by_impact[:30]
    
    sample_pool = {}
    for t in recent_tweets + top_tweets:
        key = t.get('tweet_id', t.get('full_text')[:50])
        sample_pool[key] = t
    
    final_samples = list(sample_pool.values())
    print(f"      [æ·±åº¦æŠ¥å‘Šé‡‡æ ·] ç²¾é€‰ {len(final_samples)} æ¡æ¨æ–‡...")

    input_text = ""
    for idx, t in enumerate(final_samples):
        clean_text = t.get('full_text', '').replace('\n', ' ').strip()
        if len(clean_text) > 5:
            input_text += f"[{idx+1}] {clean_text}\n"

    prompt = f"""
    ä½ æ˜¯ä¸€åé«˜çº§æƒ…æŠ¥åˆ†æä¸“å®¶ã€‚ç›®æ ‡å¯¹è±¡æ˜¯ï¼š"{name}"ã€‚
    è¨€è®ºæ ·æœ¬ï¼š{input_text}
    
    ä»»åŠ¡ï¼šè¯·ç”Ÿæˆã€Šäººç‰©æ·±åº¦ä¾§å†™ä¸è„†å¼±ç‚¹ç ”åˆ¤æŠ¥å‘Šã€‹åŠé…å¥—å›¾è¡¨æ•°æ®ã€‚

    ã€ä»»åŠ¡ä¸€ï¼š9ç»´æŠ¥å‘Šã€‘
    1. å¤§äº”äººæ ¼ 2. äººæ ¼ç¼ºé™· 3. è®¤çŸ¥å€¾å‘ 4. è¡Œä¸ºå±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ 5. ç«‹åœºå±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ 
    6. èƒ½åŠ›å±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ 7. å¿ƒæ™ºå±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ 8. éšè—æ„å›¾ 9. é¢†åŸŸè¯é¢˜
    *è¦æ±‚ï¼šç¦æ­¢å¼•ç”¨ç¼–å·ï¼Œå¤–è¯­é™„ä¸­æ–‡ç¿»è¯‘ã€‚*

    ã€ä»»åŠ¡äºŒï¼šå¯¹åç«‹åœºçŸ©é˜µã€‘
    Xè½´: 0=è´Ÿé¢, 1=ä¸­ç«‹, 2=æ­£é¢; Yè½´: 0=æ”¿æ²», 1=å†›äº‹, 2=ç»æµ, 3=æ–‡åŒ–; Value: 0-10
    
    ã€ä»»åŠ¡ä¸‰ï¼šå½±å“åŠ›ç±»å‹ã€‘
    æƒå¨, åŒä¼´, äº²æƒ… (æ€»å’Œ100)

    è¾“å‡º JSONï¼š
    {{
        "report": [ {{ "dimension": "1. å¤§äº”äººæ ¼", "summary": "...", "detail": "..." }}, ... ],
        "stance_matrix": [[0,0,8]...],
        "influence_type": [{{ "name": "æƒå¨", "value": 70 }}...]
    }}
    """

    try:
        response = requests.post(API_URL, json={
            "model": MODEL_NAME,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "response_format": {"type": "json_object"}
        }, headers={"Authorization": f"Bearer {API_KEY}"}, timeout=120)
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            content = content.replace('```json', '').replace('```', '').strip()
            try: return json.loads(content)
            except: return None
    except: pass
    return None

def update_list_json(region, summary_obj):
    if os.path.exists(LIST_FILE):
        with open(LIST_FILE, 'r', encoding='utf-8') as f:
            list_data = json.load(f)
    else:
        list_data = {}
    
    if region not in list_data:
        list_data[region] = {"region": region, "targets": []}
    
    targets = list_data[region]['targets']
    found = False
    for i, t in enumerate(targets):
        if t['id'] == summary_obj['id']:
            targets[i] = summary_obj
            found = True
            break
    if not found: targets.append(summary_obj)
        
    with open(LIST_FILE, 'w', encoding='utf-8') as f:
        json.dump(list_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… ç´¢å¼• list.json å·²æ›´æ–°")

def main():
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œå•ç›®æ ‡å…¨ç»´åº¦åˆ†æ | ç›®æ ‡: {TARGET_NAME}")
    
    if not os.path.exists(DETECT_DB_DIR): os.makedirs(DETECT_DB_DIR)
    if not os.path.exists(DETAILS_DIR): os.makedirs(DETAILS_DIR)

    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        targets_config = json.load(f)
    target_config = next((item for item in targets_config if item["name"] == TARGET_NAME), None)
    if not target_config: return

    filename = target_config.get('filename')
    region = target_config.get('region')
    category = target_config.get('category')
    file_path = os.path.join(PROFILE_DIR, filename)

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            tweets = json.load(f)
            if not isinstance(tweets, list): tweets = [tweets]
    except: return

    print(f"ğŸ”„ [æ·±åº¦åˆ†æ] æ­£åœ¨ç ”åˆ¤: {TARGET_NAME} ...")
    
    # 1. ç”Ÿæˆå®è§‚æŠ¥å‘Š
    analysis_result = generate_deep_report(TARGET_NAME, tweets)
    
    if analysis_result:
        daily_cnt = calculate_stats(tweets)

        # 2. ã€æ ¸å¿ƒä¿®æ”¹ã€‘æå–æœ€æ–°çš„ 20 æ¡æ¨æ–‡å¹¶è¿›è¡Œç¿»è¯‘å’Œç«‹åœºåˆ¤å®š
        sorted_all_tweets = sorted(tweets, key=lambda x: x.get('created_at', ''), reverse=True)
        top_20_tweets = sorted_all_tweets[:20] # åªå–20æ¡
        
        print(f"ğŸ”„ [å¾®è§‚åˆ†æ] æ­£åœ¨é€æ¡ç ”åˆ¤æœ€æ–° 20 æ¡æ¨æ–‡ (ç¿»è¯‘+ç«‹åœº)...")
        enriched_tweets = batch_analyze_tweets(top_20_tweets)

        final_detail_data = {
            "id": filename,
            "_fingerprint": get_file_fingerprint(file_path),
            "name": TARGET_NAME,
            "username": tweets[0].get('username', 'unknown'),
            "category": category,
            "daily_count": daily_cnt,
            "analysis_report": analysis_result.get("report", []),
            "stance_matrix": analysis_result.get("stance_matrix", []),
            "influence_type": analysis_result.get("influence_type", []),
            "all_tweets": enriched_tweets # è¿™é‡Œç°åœ¨æ˜¯åŒ…å« translation å’Œ stance çš„å¯Œæ•°æ®
        }
        
        detail_out_path = os.path.join(DETAILS_DIR, filename)
        with open(detail_out_path, 'w', encoding='utf-8') as f:
            json.dump(final_detail_data, f, ensure_ascii=False, indent=2)
        print(f"   âœ… è¯¦æƒ…æ–‡ä»¶ç”Ÿæˆå®Œæ¯•")

        summary_obj = {
            "id": filename,
            "name": TARGET_NAME,
            "username": final_detail_data['username'],
            "category": category,
            "daily_count": daily_cnt,
            "preview": analysis_result.get("report", [{}])[0].get("summary", "æš‚æ— æ‘˜è¦")
        }
        update_list_json(region, summary_obj)
        
    else:
        print("âŒ LLM åˆ†æå¤±è´¥ã€‚")

if __name__ == "__main__":
    main()