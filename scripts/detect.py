import json
import os
import requests
import hashlib
import time
import random
from datetime import datetime
from dateutil import parser
import traceback

# ================= é…ç½®åŒºåŸŸ =================
API_KEY = "sk-7ba052d40efe48ae990141e577d952d1"
API_URL = "https://api.deepseek.com/chat/completions"
MODEL_NAME = "deepseek-chat"

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
PROFILE_DIR = os.path.join(BASE_DIR, 'database', 'raw', 'profile')
CONFIG_FILE = os.path.join(PROFILE_DIR, 'targets.json')

# è¾“å‡ºç›®å½•
DETECT_DB_DIR = os.path.join(BASE_DIR, 'public', 'db', 'detect')
LIST_FILE = os.path.join(DETECT_DB_DIR, 'list.json')
DETAILS_DIR = os.path.join(DETECT_DB_DIR, 'details')
# ===========================================

def get_file_fingerprint(file_path):
    if not os.path.exists(file_path): return None
    stat = os.stat(file_path)
    identifier = f"{os.path.basename(file_path)}_{stat.st_size}_{stat.st_mtime}"
    return hashlib.md5(identifier.encode('utf-8')).hexdigest()

def calculate_stats(tweets):
    if not tweets: return 0
    dates = []
    for t in tweets:
        try:
            dt = parser.parse(t.get('created_at', ''))
            dates.append(dt)
        except: continue
    if not dates: return 0
    delta_days = (max(dates) - min(dates)).days
    if delta_days < 1: delta_days = 1
    return round(len(tweets) / delta_days, 1)

def batch_analyze_tweets(tweets):
    """ã€å¾®è§‚åˆ†æã€‘æ‰¹é‡åˆ†ææœ€æ–°çš„ 20 æ¡æ¨æ–‡"""
    if not tweets: return []
    
    input_text = ""
    for idx, t in enumerate(tweets):
        clean_text = t.get('full_text', '').replace('\n', ' ').strip()
        input_text += f"ID[{idx}]: {clean_text}\n"
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæƒ…æŠ¥ç¿»è¯‘å®˜ã€‚è¯·åˆ†æä»¥ä¸‹ç¤¾äº¤åª’ä½“æ¨æ–‡åˆ—è¡¨ã€‚
    
    è¾“å…¥å†…å®¹ï¼š
    {input_text}
    
    ä»»åŠ¡ï¼š
    1. ã€ä¸­æ–‡ç¿»è¯‘ã€‘ï¼šå°†æ¨æ–‡ç¿»è¯‘æˆæµç•…çš„ä¸­æ–‡ã€‚
    2. ã€å¯¹ä¸­ç«‹åœºã€‘ï¼šåˆ¤æ–­è¯¥æ¡æ¨æ–‡ä½“ç°çš„å¯¹åç«‹åœºï¼ˆè‹¥æ¨æ–‡ä¸ä¸­å›½æ— å…³ï¼Œæ ‡è®°ä¸ºâ€œæ— å…³â€ï¼‰ã€‚
       ç«‹åœºé€‰é¡¹ï¼šæ­£é¢ (Positive)ã€ä¸­ç«‹ (Neutral)ã€è´Ÿé¢ (Negative)ã€æ— å…³ (Irrelevant)ã€‚
    
    è¾“å‡ºè¦æ±‚ï¼š
    è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼Œé¡ºåºä¸è¾“å…¥ ID ä¸¥æ ¼å¯¹åº”ã€‚æ ¼å¼å¦‚ä¸‹ï¼š
    [
        {{ "id": 0, "trans": "ä¸­æ–‡ç¿»è¯‘å†…å®¹...", "stance": "è´Ÿé¢" }},
        {{ "id": 1, "trans": "ä¸­æ–‡ç¿»è¯‘...", "stance": "æ— å…³" }}
    ]
    """
    
    try:
        response = requests.post(API_URL, json={
            "model": MODEL_NAME,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "response_format": {"type": "json_object"}
        }, headers={"Authorization": f"Bearer {API_KEY}"}, timeout=120)
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            content = content.replace('```json', '').replace('```', '').strip()
            try:
                raw_json = json.loads(content)
                result_list = []
                if isinstance(raw_json, dict):
                    for k, v in raw_json.items():
                        if isinstance(v, list): result_list = v
                elif isinstance(raw_json, list):
                    result_list = raw_json
                
                enriched_tweets = []
                analysis_map = {item['id']: item for item in result_list}
                for idx, t in enumerate(tweets):
                    analysis = analysis_map.get(idx, {"trans": "ç¿»è¯‘å¤±è´¥", "stance": "ä¸­ç«‹"})
                    enriched_tweets.append({
                        "created_at": t.get('created_at'),
                        "text": t.get('full_text'),
                        "translation": analysis.get('trans'),
                        "stance": analysis.get('stance'),
                        "metrics": {
                            "reply": t.get('reply_count', 0),
                            "retweet": t.get('retweet_count', 0),
                            "like": t.get('favorite_count', 0)
                        }
                    })
                return enriched_tweets
            except:
                print("âŒ æ¨æ–‡æ‰¹é‡åˆ†æ JSON è§£æå¤±è´¥")
    except Exception as e:
        print(f"API Error (Batch Analysis): {e}")
    return [] 

def generate_deep_report(name, raw_tweets):
    """ã€å®è§‚åˆ†æã€‘ç”Ÿæˆ 9 ç»´æŠ¥å‘Š + çŸ©é˜µ + é¥¼å›¾"""
    def safe_parse_time(t):
        try: return parser.parse(t.get('created_at', ''))
        except: return datetime.min

    sorted_by_date = sorted(raw_tweets, key=safe_parse_time, reverse=True)
    recent_tweets = sorted_by_date[:20]
    
    def get_impact(t): return (t.get('retweet_count',0)*2 + t.get('reply_count',0))
    sorted_by_impact = sorted(raw_tweets, key=get_impact, reverse=True)
    top_tweets = sorted_by_impact[:30]
    
    sample_pool = {}
    for t in recent_tweets + top_tweets:
        key = t.get('tweet_id', t.get('full_text')[:50])
        sample_pool[key] = t
    
    final_samples = list(sample_pool.values())
    
    input_text = ""
    for idx, t in enumerate(final_samples):
        clean_text = t.get('full_text', '').replace('\n', ' ').strip()
        if len(clean_text) > 5:
            input_text += f"[{idx+1}] {clean_text}\n"

    prompt = f"""
    ä½ æ˜¯ä¸€åé«˜çº§æƒ…æŠ¥åˆ†æä¸“å®¶ã€‚ç›®æ ‡å¯¹è±¡æ˜¯ï¼š"{name}"ã€‚
    è¨€è®ºæ ·æœ¬ï¼š{input_text}
    
    ä»»åŠ¡ï¼šè¯·ç”Ÿæˆã€Šäººç‰©æ·±åº¦ä¾§å†™ä¸è„†å¼±ç‚¹ç ”åˆ¤æŠ¥å‘Šã€‹åŠé…å¥—å›¾è¡¨æ•°æ®ã€‚

    ã€ä»»åŠ¡ä¸€ï¼š9ç»´æŠ¥å‘Šã€‘
    1. å¤§äº”äººæ ¼ 2. äººæ ¼ç¼ºé™· 3. è®¤çŸ¥å€¾å‘ 4. è¡Œä¸ºå±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ 5. ç«‹åœºå±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ 
    6. èƒ½åŠ›å±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ 7. å¿ƒæ™ºå±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ 8. éšè—æ„å›¾ 9. é¢†åŸŸè¯é¢˜
    *è¦æ±‚ï¼šç¦æ­¢å¼•ç”¨ç¼–å·ï¼Œå¤–è¯­é™„ä¸­æ–‡ç¿»è¯‘ã€‚*

    ã€ä»»åŠ¡äºŒï¼šå¯¹åç«‹åœºçŸ©é˜µã€‘
    Xè½´: 0=è´Ÿé¢, 1=ä¸­ç«‹, 2=æ­£é¢; Yè½´: 0=æ”¿æ²», 1=å†›äº‹, 2=ç»æµ, 3=æ–‡åŒ–; Value: 0-10
    
    ã€ä»»åŠ¡ä¸‰ï¼šå½±å“åŠ›ç±»å‹ã€‘
    æƒå¨, åŒä¼´, äº²æƒ… (æ€»å’Œ100)

    è¾“å‡º JSONï¼š
    {{
        "report": [ {{ "dimension": "1. å¤§äº”äººæ ¼", "summary": "...", "detail": "..." }}, ... ],
        "stance_matrix": [[0,0,8]...],
        "influence_type": [{{ "name": "æƒå¨", "value": 70 }}...]
    }}
    """

    try:
        response = requests.post(API_URL, json={
            "model": MODEL_NAME,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "response_format": {"type": "json_object"}
        }, headers={"Authorization": f"Bearer {API_KEY}"}, timeout=120)
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            content = content.replace('```json', '').replace('```', '').strip()
            try: return json.loads(content)
            except: return None
    except: pass
    return None

def update_list_json(region, summary_obj):
    if os.path.exists(LIST_FILE):
        with open(LIST_FILE, 'r', encoding='utf-8') as f:
            list_data = json.load(f)
    else:
        list_data = {}
    
    if region not in list_data:
        list_data[region] = {"region": region, "targets": []}
    
    targets = list_data[region]['targets']
    found = False
    for i, t in enumerate(targets):
        if t['id'] == summary_obj['id']:
            targets[i] = summary_obj
            found = True
            break
    if not found: targets.append(summary_obj)
        
    with open(LIST_FILE, 'w', encoding='utf-8') as f:
        json.dump(list_data, f, ensure_ascii=False, indent=2)

def process_single_target(target_config, index, total):
    """å¤„ç†å•ä¸ªç›®æ ‡ï¼ŒåŒ…å«è·³è¿‡é€»è¾‘"""
    target_name = target_config.get('name')
    filename = target_config.get('filename')
    region = target_config.get('region')
    category = target_config.get('category')
    
    # ---------------------------------------------------------
    # ğŸ” æ£€æŸ¥ç‚¹ 1ï¼šæ£€æŸ¥æ˜¯å¦å·²ç»ç”Ÿæˆè¿‡è¯¦æƒ…æ–‡ä»¶
    # ---------------------------------------------------------
    detail_out_path = os.path.join(DETAILS_DIR, filename)
    if os.path.exists(detail_out_path):
        print(f"[{index}/{total}] â© å·²å­˜åœ¨ï¼Œè·³è¿‡: {target_name} ({filename})")
        return "SKIPPED"

    # ---------------------------------------------------------
    # ğŸ” æ£€æŸ¥ç‚¹ 2ï¼šæ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆä½ ä¹‹å‰å¯èƒ½åˆ é™¤äº†ï¼‰
    # ---------------------------------------------------------
    file_path = os.path.join(PROFILE_DIR, filename)
    if not os.path.exists(file_path):
        print(f"[{index}/{total}] âš ï¸ æºæ–‡ä»¶ç¼ºå¤±ï¼Œè·³è¿‡: {filename}")
        return "MISSING_SOURCE"

    # =========================================================
    # ğŸš€ å¼€å§‹å¤„ç†
    # =========================================================
    print(f"\n[{index}/{total}] ğŸš€ å¼€å§‹ç ”åˆ¤: {target_name} ({filename})")

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            tweets = json.load(f)
            if not isinstance(tweets, list): tweets = [tweets]
    except Exception as e:
        print(f"   âŒ æ–‡ä»¶æ ¼å¼é”™è¯¯: {filename}")
        return False

    # 1. ç”Ÿæˆå®è§‚æŠ¥å‘Š
    print(f"   ğŸ”„ [1/2] ç”Ÿæˆæ·±åº¦ç”»åƒæŠ¥å‘Š...")
    analysis_result = generate_deep_report(target_name, tweets)
    
    if analysis_result:
        daily_cnt = calculate_stats(tweets)

        # 2. æ‰¹é‡åˆ†ææœ€æ–° 20 æ¡
        print(f"   ğŸ”„ [2/2] é€æ¡åˆ†ææœ€æ–°æ¨æ–‡...")
        sorted_all_tweets = sorted(tweets, key=lambda x: x.get('created_at', ''), reverse=True)
        top_20_tweets = sorted_all_tweets[:20]
        enriched_tweets = batch_analyze_tweets(top_20_tweets)

        final_detail_data = {
            "id": filename,
            "_fingerprint": get_file_fingerprint(file_path),
            "name": target_name,
            "username": tweets[0].get('username', 'unknown'),
            "category": category,
            "daily_count": daily_cnt,
            "analysis_report": analysis_result.get("report", []),
            "stance_matrix": analysis_result.get("stance_matrix", []),
            "influence_type": analysis_result.get("influence_type", []),
            "all_tweets": enriched_tweets
        }
        
        with open(detail_out_path, 'w', encoding='utf-8') as f:
            json.dump(final_detail_data, f, ensure_ascii=False, indent=2)
        
        summary_obj = {
            "id": filename,
            "name": target_name,
            "username": final_detail_data['username'],
            "category": category,
            "daily_count": daily_cnt,
            "preview": analysis_result.get("report", [{}])[0].get("summary", "æš‚æ— æ‘˜è¦")
        }
        update_list_json(region, summary_obj)
        print(f"   âœ… å¤„ç†æˆåŠŸï¼æ•°æ®åº“å·²æ›´æ–°")
        return "SUCCESS"
    else:
        print(f"   âŒ LLM åˆ†æå¤±è´¥")
        return False

def main():
    print(f"ğŸ”¥ å¯åŠ¨å¢é‡è‡ªåŠ¨åŒ–ç ”åˆ¤ç¨‹åº (è‡ªåŠ¨è·³è¿‡å·²å­˜åœ¨/å·²åˆ é™¤é¡¹)")
    
    if not os.path.exists(DETECT_DB_DIR): os.makedirs(DETECT_DB_DIR)
    if not os.path.exists(DETAILS_DIR): os.makedirs(DETAILS_DIR)

    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            targets_config = json.load(f)
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å– targets.json: {e}")
        return

    total_targets = len(targets_config)
    print(f"ğŸ“„ targets.json ä¸­å…±æœ‰ {total_targets} ä¸ªé…ç½®é¡¹ï¼Œå‡†å¤‡æ‰«æ...\n")

    processed_count = 0
    skipped_count = 0
    
    for i, target_config in enumerate(targets_config, 1):
        try:
            status = process_single_target(target_config, i, total_targets)
            
            if status == "SUCCESS":
                processed_count += 1
                # åªæœ‰çœŸæ­£å¤„ç†äº†æ‰éœ€è¦å†·å´ï¼Œè·³è¿‡çš„ä¸éœ€è¦å†·å´
                sleep_time = random.randint(2, 5)
                print(f"   ğŸ’¤ å†·å´ {sleep_time} ç§’...")
                time.sleep(sleep_time)
            elif status == "SKIPPED" or status == "MISSING_SOURCE":
                skipped_count += 1
                # è·³è¿‡æ—¶æ— éœ€ç­‰å¾…ï¼Œç›´æ¥ä¸‹ä¸€ä¸ª
                
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·ç»ˆæ­¢")
            break
        except Exception as e:
            print(f"   âŒ ç³»ç»Ÿçº§é”™è¯¯: {e}")
            traceback.print_exc()

    print(f"\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"ğŸ‰ ä»»åŠ¡ç»“æŸï¼")
    print(f"   - æ–°å¢å¤„ç†: {processed_count}")
    print(f"   - è‡ªåŠ¨è·³è¿‡: {skipped_count}")

if __name__ == "__main__":
    main()