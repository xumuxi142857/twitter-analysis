import json
import os
import requests
import hashlib
from datetime import datetime
from dateutil import parser
import traceback

# ================= é…ç½®åŒºåŸŸ =================
# ğŸ¯ åœ¨è¿™é‡Œå¡«å†™ä½ è¦å¤„ç†çš„ç›®æ ‡åç§°
TARGET_NAME = "asahi"  

API_KEY = "sk-7ba052d40efe48ae990141e577d952d1"  # 
API_URL = "https://api.deepseek.com/chat/completions"
MODEL_NAME = "deepseek-chat"  # 

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
PROFILE_DIR = os.path.join(BASE_DIR, 'database', 'raw', 'profile')
CONFIG_FILE = os.path.join(PROFILE_DIR, 'targets.json')

# è¾“å‡ºç›®å½•
DETECT_DB_DIR = os.path.join(BASE_DIR, 'public', 'db', 'detect')
LIST_FILE = os.path.join(DETECT_DB_DIR, 'list.json')
DETAILS_DIR = os.path.join(DETECT_DB_DIR, 'details')
# ===========================================

def get_file_fingerprint(file_path):
    if not os.path.exists(file_path): return None
    stat = os.stat(file_path)
    identifier = f"{os.path.basename(file_path)}_{stat.st_size}_{stat.st_mtime}"
    return hashlib.md5(identifier.encode('utf-8')).hexdigest()

def calculate_stats(tweets):
    if not tweets: return 0
    dates = []
    for t in tweets:
        try:
            dt = parser.parse(t.get('created_at', ''))
            dates.append(dt)
        except: continue
    if not dates: return 0
    delta_days = (max(dates) - min(dates)).days
    if delta_days < 1: delta_days = 1
    return round(len(tweets) / delta_days, 1)

def generate_deep_report(name, raw_tweets):
    """
    ç”Ÿæˆï¼š9ç»´æŠ¥å‘Š + ç«‹åœºçŸ©é˜µ + å½±å“åŠ›é¥¼å›¾
    """
    if not raw_tweets: return None

    # --- 1. æ•°æ®é‡‡æ · ---
    def safe_parse_time(t):
        try: return parser.parse(t.get('created_at', ''))
        except: return datetime.min

    sorted_by_date = sorted(raw_tweets, key=safe_parse_time, reverse=True)
    recent_tweets = sorted_by_date[:20]
    
    def get_impact(t): return (t.get('retweet_count',0)*2 + t.get('reply_count',0))
    sorted_by_impact = sorted(raw_tweets, key=get_impact, reverse=True)
    top_tweets = sorted_by_impact[:30]
    
    # åˆå¹¶å»é‡
    sample_pool = {}
    for t in recent_tweets + top_tweets:
        key = t.get('tweet_id', t.get('full_text')[:50])
        sample_pool[key] = t
    
    final_samples = list(sample_pool.values())
    print(f"      [é‡‡æ ·] ç²¾é€‰ {len(final_samples)} æ¡æ¨æ–‡è¿›è¡Œæ·±åº¦ç”»åƒ...")

    input_text = ""
    for idx, t in enumerate(final_samples):
        clean_text = t.get('full_text', '').replace('\n', ' ').strip()
        if len(clean_text) > 5:
            input_text += f"[{idx+1}] {clean_text}\n"

    # --- 2. å¤åˆ Prompt ---
    prompt = f"""
    ä½ æ˜¯ä¸€åé«˜çº§æƒ…æŠ¥åˆ†æä¸“å®¶ã€‚ç›®æ ‡å¯¹è±¡æ˜¯ï¼š"{name}"ã€‚
    ä»¥ä¸‹æ˜¯è¯¥ç›®æ ‡åœ¨ç¤¾äº¤åª’ä½“ä¸Šçš„è¨€è®ºæ ·æœ¬ï¼š
    {input_text}
    
    ä»»åŠ¡ï¼šè¯·åŸºäºä¸Šè¿°æ•°æ®ï¼Œå®Œæˆä»¥ä¸‹ä¸‰é¡¹åˆ†æä»»åŠ¡ï¼Œå¹¶ä»¥ä¸¥æ ¼çš„ JSON æ ¼å¼è¾“å‡ºã€‚

    ã€ä»»åŠ¡ä¸€ï¼šæ·±åº¦ç ”åˆ¤æŠ¥å‘Š (Report)ã€‘
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ 9 ä¸ªç»´åº¦è¿›è¡Œåˆ†æã€‚æ¯é¡¹åŒ…å« title, summary(30å­—å†…), detail(150å­—å·¦å³)ã€‚
    1. å¤§äº”äººæ ¼ (Big Five): åˆ†æå¼€æ”¾æ€§ã€å°½è´£æ€§ã€å¤–å‘æ€§ã€å®œäººæ€§ã€ç¥ç»è´¨çš„ç‰¹å¾ã€‚
    2. äººæ ¼ç¼ºé™· (Personality Defects): è¯†åˆ«å¦‚è‡ªæ‹ã€é©¬åŸºé›…ç»´åˆ©ä¸»ä¹‰ã€å†·æ¼ ç­‰æš—é»‘ç‰¹å¾ã€‚
    3. è®¤çŸ¥å€¾å‘ (Cognitive Bias): åˆ†æé˜´è°‹æ€ç»´ã€å½’å› åå·®ã€åˆ»æ¿å°è±¡ç­‰ã€‚
    4. è¡Œä¸ºå±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ (Behavioral Vulnerabilities): è¯†åˆ«å†²åŠ¨ã€å›é¿è´£ä»»ã€æ“æ§ç­‰è¡Œä¸ºå¼±ç‚¹ã€‚
    5. ç«‹åœºå±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ (Stance Vulnerabilities): è¯†åˆ«ç«‹åœºæ‘‡æ‘†ã€è¿åˆã€æ¨¡ç³Šç­‰é—®é¢˜ã€‚
    6. èƒ½åŠ›å±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ (Competence Vulnerabilities): è¯„ä¼°å¤–äº¤ã€ç»æµã€ç®¡ç†ç­‰æ–¹é¢çš„çŸ­æ¿ã€‚
    7. å¿ƒæ™ºå±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ (Mental Vulnerabilities): åˆ†ææƒ…ç»ªç¨³å®šæ€§ã€åæ‰§ã€é£é™©åå¥½ç­‰ã€‚
    8. éšè—æ„å›¾ (Hidden Intentions): æ¨æµ‹å…¶å¯¹ä¸åŒåˆ©ç›Šæ–¹ï¼ˆå¦‚æœ¬å›½ã€ç›Ÿå‹ã€å¯¹æ‰‹ï¼‰çš„çœŸå®æ„å›¾ã€‚
    9. é¢†åŸŸè¯é¢˜ (Domain Topics): æ€»ç»“å…¶å…³æ³¨çš„æ ¸å¿ƒé¢†åŸŸï¼ˆæ”¿æ²»ã€ç»æµã€å†›äº‹ç­‰ï¼‰åŠå…·ä½“å­è¯é¢˜ã€‚
    *è¦æ±‚ï¼šç¦æ­¢å¼•ç”¨æ ·æœ¬ç¼–å·ï¼Œé‡åˆ°å¤–è¯­åè¯éœ€é™„ä¸­æ–‡ç¿»è¯‘ã€‚*

    ã€ä»»åŠ¡äºŒï¼šå¯¹åç«‹åœºçŸ©é˜µ (Stance Matrix)ã€‘
    è¯„ä¼°å…¶å¯¹ä¸­å›½çš„æ€åº¦ã€‚
    ç»´åº¦(Yè½´): 0=æ”¿æ²», 1=å†›äº‹, 2=ç»æµ, 3=æ–‡åŒ–
    ç«‹åœº(Xè½´): 0=è´Ÿé¢(åå/å¼ºç¡¬), 1=ä¸­ç«‹/åŠ¡å®, 2=æ­£é¢(å‹å¥½/åˆä½œ)
    æ•°å€¼(Value): 0-10 (å¼ºåº¦)
    æ ¼å¼ï¼š[[x, y, value], [x, y, value]...] (éœ€è¦†ç›–æ‰€æœ‰4ä¸ªç»´åº¦)

    ã€ä»»åŠ¡ä¸‰ï¼šå½±å“åŠ›ç±»å‹ (Influence Type)ã€‘
    è¯„ä¼°å…¶å½±å“å—ä¼—çš„æ–¹å¼ï¼Œæ€»å’Œ 100ã€‚
    ç±»å‹ï¼šæƒå¨ (Authority), åŒä¼´ (Peer), äº²æƒ… (Kinship)
    æ ¼å¼ï¼š[{{ "name": "æƒå¨", "value": 60 }}, ...]

    â­â­ è¾“å‡º JSON ç»“æ„è¦æ±‚ â­â­ï¼š
    {{
        "report": [ {{ "dimension": "1. å¤§äº”äººæ ¼", "summary": "...", "detail": "..." }}, ... ],
        "stance_matrix": [[0,0,8], [1,0,5], [1,2,4], ...],
        "influence_type": [{{ "name": "æƒå¨", "value": 70 }}, {{ "name": "åŒä¼´", "value": 20 }}, {{ "name": "äº²æƒ…", "value": 10 }}]
    }}
    """

    try:
        response = requests.post(API_URL, json={
            "model": MODEL_NAME,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "response_format": {"type": "json_object"}
        }, headers={"Authorization": f"Bearer {API_KEY}"}, timeout=120)
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            content = content.replace('```json', '').replace('```', '').strip()
            
            try:
                return json.loads(content)
            except:
                print(f"âŒ JSON è§£æå¤±è´¥ã€‚")
                return None
    except Exception as e:
        print(f"API Error: {e}")
    
    return None

def update_list_json(region, summary_obj):
    if os.path.exists(LIST_FILE):
        with open(LIST_FILE, 'r', encoding='utf-8') as f:
            list_data = json.load(f)
    else:
        list_data = {}
    
    if region not in list_data:
        list_data[region] = {"region": region, "targets": []}
    
    targets = list_data[region]['targets']
    found = False
    for i, t in enumerate(targets):
        if t['id'] == summary_obj['id']:
            targets[i] = summary_obj
            found = True
            break
    
    if not found: targets.append(summary_obj)
        
    with open(LIST_FILE, 'w', encoding='utf-8') as f:
        json.dump(list_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… ç´¢å¼• list.json å·²æ›´æ–°: {summary_obj['name']}")

def main():
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œå•ç›®æ ‡å…¨ç»´åº¦åˆ†æ | ç›®æ ‡: {TARGET_NAME}")
    
    if not os.path.exists(DETECT_DB_DIR): os.makedirs(DETECT_DB_DIR)
    if not os.path.exists(DETAILS_DIR): os.makedirs(DETAILS_DIR)

    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        targets_config = json.load(f)
    
    target_config = next((item for item in targets_config if item["name"] == TARGET_NAME), None)
    
    if not target_config:
        print(f"âŒ æœªæ‰¾åˆ° '{TARGET_NAME}' é…ç½®ã€‚")
        return

    filename = target_config.get('filename')
    region = target_config.get('region')
    category = target_config.get('category')
    file_path = os.path.join(PROFILE_DIR, filename)

    if not os.path.exists(file_path):
        print(f"âŒ æ‰¾ä¸åˆ°æºæ–‡ä»¶: {file_path}")
        return

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            tweets = json.load(f)
            if not isinstance(tweets, list): tweets = [tweets]
    except Exception as e:
        print(f"âŒ è¯»å–æº JSON å¤±è´¥: {e}")
        return

    print(f"ğŸ”„ [æ·±åº¦åˆ†æ] æ­£åœ¨ç ”åˆ¤: {TARGET_NAME} ...")
    
    # è·å–ç»¼åˆåˆ†æç»“æœ
    analysis_result = generate_deep_report(TARGET_NAME, tweets)
    
    if analysis_result:
        daily_cnt = calculate_stats(tweets)

        # æ•´ç†æ¨æ–‡ (Top 100)
        clean_tweets = []
        sorted_all_tweets = sorted(tweets, key=lambda x: x.get('created_at', ''), reverse=True)
        for t in sorted_all_tweets[:100]:
            clean_tweets.append({
                "created_at": t.get('created_at'),
                "text": t.get('full_text'),
                "metrics": {
                    "reply": t.get('reply_count', 0),
                    "retweet": t.get('retweet_count', 0),
                    "like": t.get('favorite_count', 0)
                }
            })

        final_detail_data = {
            "id": filename,
            "_fingerprint": get_file_fingerprint(file_path),
            "name": TARGET_NAME,
            "username": tweets[0].get('username', 'unknown'),
            "category": category,
            "daily_count": daily_cnt,
            "analysis_report": analysis_result.get("report", []), # 9ç‚¹æŠ¥å‘Š
            "stance_matrix": analysis_result.get("stance_matrix", []), # ç«‹åœºçŸ©é˜µ
            "influence_type": analysis_result.get("influence_type", []), # å½±å“åŠ›é¥¼å›¾
            "all_tweets": clean_tweets
        }
        
        detail_out_path = os.path.join(DETAILS_DIR, filename)
        with open(detail_out_path, 'w', encoding='utf-8') as f:
            json.dump(final_detail_data, f, ensure_ascii=False, indent=2)
        print(f"   âœ… è¯¦æƒ…æ–‡ä»¶ç”Ÿæˆå®Œæ¯• (åŒ…å«å›¾è¡¨æ•°æ®)")

        summary_obj = {
            "id": filename,
            "name": TARGET_NAME,
            "username": final_detail_data['username'],
            "category": category,
            "daily_count": daily_cnt,
            # å–ç¬¬ä¸€æ¡æ‘˜è¦ä½œä¸ºé¢„è§ˆ
            "preview": analysis_result.get("report", [{}])[0].get("summary", "æš‚æ— æ‘˜è¦")
        }
        update_list_json(region, summary_obj)
        
    else:
        print("âŒ LLM åˆ†æå¤±è´¥ã€‚")

if __name__ == "__main__":
    main()