import json
import os
import requests
import hashlib
from datetime import datetime
from dateutil import parser
import traceback

# ================= é…ç½®åŒºåŸŸ =================
# ğŸ¯ ç›®æ ‡åç§° (å¿…é¡»ä¸ targets.json ä¸€è‡´)
TARGET_NAME = "asahi" 

API_KEY = "sk-7ba052d40efe48ae990141e577d952d1"  # 
API_URL = "https://api.deepseek.com/chat/completions"
MODEL_NAME = "deepseek-chat"  # 

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
PROFILE_DIR = os.path.join(BASE_DIR, 'database', 'raw', 'profile')
CONFIG_FILE = os.path.join(PROFILE_DIR, 'targets.json')

# è¾“å‡ºç›®å½•
DETECT_DB_DIR = os.path.join(BASE_DIR, 'public', 'db', 'detect')
LIST_FILE = os.path.join(DETECT_DB_DIR, 'list.json')
DETAILS_DIR = os.path.join(DETECT_DB_DIR, 'details')
# ===========================================

def get_file_fingerprint(file_path):
    if not os.path.exists(file_path): return None
    stat = os.stat(file_path)
    identifier = f"{os.path.basename(file_path)}_{stat.st_size}_{stat.st_mtime}"
    return hashlib.md5(identifier.encode('utf-8')).hexdigest()

def calculate_stats(tweets):
    if not tweets: return 0
    dates = []
    for t in tweets:
        try:
            dt = parser.parse(t.get('created_at', ''))
            dates.append(dt)
        except: continue
    if not dates: return 0
    delta_days = (max(dates) - min(dates)).days
    if delta_days < 1: delta_days = 1
    return round(len(tweets) / delta_days, 1)

def generate_deep_report(name, raw_tweets):
    """
    ç”Ÿæˆ 9 ç»´æ·±åº¦ç ”åˆ¤æŠ¥å‘Š (ç»“æ„åŒ– JSON)
    """
    if not raw_tweets: return None

    # --- 1. æ•°æ®é‡‡æ · (ä¸ºäº†ä¸Šä¸‹æ–‡çª—å£ï¼Œç²¾é€‰é«˜ä»·å€¼æ¨æ–‡) ---
    def safe_parse_time(t):
        try: return parser.parse(t.get('created_at', ''))
        except: return datetime.min

    # å–æœ€æ–°çš„ 30 æ¡ (çœ‹è¿‘å†µ)
    sorted_by_date = sorted(raw_tweets, key=safe_parse_time, reverse=True)
    recent_tweets = sorted_by_date[:30]
    
    # å–äº’åŠ¨æœ€é«˜çš„ 40 æ¡ (çœ‹å…¸å‹ç‰¹å¾)
    def get_impact(t): return (t.get('retweet_count',0)*2 + t.get('reply_count',0))
    sorted_by_impact = sorted(raw_tweets, key=get_impact, reverse=True)
    top_tweets = sorted_by_impact[:40]
    
    # åˆå¹¶å»é‡
    sample_pool = {}
    for t in recent_tweets + top_tweets:
        key = t.get('tweet_id', t.get('full_text')[:50])
        sample_pool[key] = t
    
    final_samples = list(sample_pool.values())
    print(f"      [é‡‡æ ·] ç²¾é€‰ {len(final_samples)} æ¡æ¨æ–‡è¿›è¡Œæ·±åº¦ç”»åƒ...")

    input_text = ""
    for idx, t in enumerate(final_samples):
        clean_text = t.get('full_text', '').replace('\n', ' ').strip()
        if len(clean_text) > 5:
            input_text += f"[{idx+1}] {clean_text}\n"

    # --- 2. 9ç»´åº¦æ·±åº¦ Prompt ---
    prompt = f"""
    ä½ æ˜¯ä¸€åæ”¿æ²»å¿ƒç†å­¦ä¸æƒ…æŠ¥åˆ†æä¸“å®¶ã€‚ç›®æ ‡å¯¹è±¡æ˜¯ï¼š"{name}"ã€‚
    åŸºäºæä¾›çš„æ¨ç‰¹è¨€è®ºæ ·æœ¬ï¼Œè¯·ç”Ÿæˆä¸€ä»½ã€Šäººç‰©æ·±åº¦ä¾§å†™ä¸è„†å¼±ç‚¹ç ”åˆ¤æŠ¥å‘Šã€‹ã€‚
    
    æ¨æ–‡æ ·æœ¬ï¼š
    {input_text}
    
    ä»»åŠ¡ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ 9 ä¸ªç»´åº¦è¿›è¡Œåˆ†æã€‚å†…å®¹éœ€ä¸“ä¸šã€ç®€ç»ƒï¼ˆç±»ä¼¼ç®€æŠ¥é£æ ¼ï¼‰ï¼Œé¿å…å†—é•¿ã€‚
    
    åˆ†æç»´åº¦è¦æ±‚ï¼š
    1. å¤§äº”äººæ ¼ (Big Five): åˆ†æå¼€æ”¾æ€§ã€å°½è´£æ€§ã€å¤–å‘æ€§ã€å®œäººæ€§ã€ç¥ç»è´¨çš„ç‰¹å¾ã€‚
    2. äººæ ¼ç¼ºé™· (Personality Defects): è¯†åˆ«å¦‚è‡ªæ‹ã€é©¬åŸºé›…ç»´åˆ©ä¸»ä¹‰ã€å†·æ¼ ç­‰æš—é»‘ç‰¹å¾ã€‚
    3. è®¤çŸ¥å€¾å‘ (Cognitive Bias): åˆ†æé˜´è°‹æ€ç»´ã€å½’å› åå·®ã€åˆ»æ¿å°è±¡ç­‰ã€‚
    4. è¡Œä¸ºå±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ (Behavioral Vulnerabilities): è¯†åˆ«å†²åŠ¨ã€å›é¿è´£ä»»ã€æ“æ§ç­‰è¡Œä¸ºå¼±ç‚¹ã€‚
    5. ç«‹åœºå±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ (Stance Vulnerabilities): è¯†åˆ«ç«‹åœºæ‘‡æ‘†ã€è¿åˆã€æ¨¡ç³Šç­‰é—®é¢˜ã€‚
    6. èƒ½åŠ›å±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ (Competence Vulnerabilities): è¯„ä¼°å¤–äº¤ã€ç»æµã€ç®¡ç†ç­‰æ–¹é¢çš„çŸ­æ¿ã€‚
    7. å¿ƒæ™ºå±‚é¢è®¤çŸ¥è„†å¼±ç‚¹ (Mental Vulnerabilities): åˆ†ææƒ…ç»ªç¨³å®šæ€§ã€åæ‰§ã€é£é™©åå¥½ç­‰ã€‚
    8. éšè—æ„å›¾ (Hidden Intentions): æ¨æµ‹å…¶å¯¹ä¸åŒåˆ©ç›Šæ–¹ï¼ˆå¦‚æœ¬å›½ã€ç›Ÿå‹ã€å¯¹æ‰‹ï¼‰çš„çœŸå®æ„å›¾ã€‚
    9. é¢†åŸŸè¯é¢˜ (Domain Topics): æ€»ç»“å…¶å…³æ³¨çš„æ ¸å¿ƒé¢†åŸŸï¼ˆæ”¿æ²»ã€ç»æµã€å†›äº‹ç­‰ï¼‰åŠå…·ä½“å­è¯é¢˜ã€‚

    è¾“å‡ºæ ¼å¼ (Strict JSON Array):
    è¿”å›ä¸€ä¸ªåŒ…å« 9 ä¸ªå¯¹è±¡çš„æ•°ç»„ã€‚æ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š
    - "dimension": ç»´åº¦åç§° (ä¾‹å¦‚ "1. å¤§äº”äººæ ¼")
    - "summary": è¯¥ç»´åº¦çš„æ•´ä½“ä¸€å¥è¯ç»¼è¿° (50å­—å†…)
    - "sub_items": ä¸€ä¸ªæ•°ç»„ï¼ŒåŒ…å«å…·ä½“åˆ†æç‚¹ã€‚æ¯ä¸ªç‚¹åŒ…å« "term"(å…³é”®è¯/å­ç»´åº¦) å’Œ "analysis"(å…·ä½“è¡¨ç°ä¸æ¨è®ºï¼Œ100å­—å·¦å³),å¦å¤–æ³¨æ„analysisä¸­ä¸è¦å‡ºç° #1ï¼Œ#2è¿™ç§å¼•ç”¨æ ·æœ¬çš„è¯´æ³•ï¼Œä¸éœ€è¦å†™ä»ä»€ä¹ˆåœ°æ–¹å¾—å‡ºç»“è®ºã€‚

    JSON ç»“æ„ç¤ºä¾‹ (è¯·ä¸¥æ ¼éµå®ˆ):
    [
      {{
        "dimension": "1. å¤§äº”äººæ ¼",
        "summary": "æ•´ä½“è¡¨ç°ä¸ºé«˜å¼€æ”¾æ€§ã€ä½å®œäººæ€§ï¼Œæƒ…ç»ªç¨³å®šæ€§è¾ƒå·®ã€‚",
        "sub_items": [
          {{ "term": "å¼€æ”¾æ€§", "analysis": "è¡¨ç°ç‰¹å¾ï¼šè¾ƒé«˜ã€‚æ¨æ–‡æ¶‰è¶³å¤šå…ƒè®®é¢˜... æ¨ç†ä¾æ®ï¼šå¤šæ¬¡å¼•ç”¨..." }},
          {{ "term": "ç¥ç»è´¨", "analysis": "è¡¨ç°ç‰¹å¾ï¼šä¸­é«˜ã€‚é¢å¯¹æ‰¹è¯„ååº”æ¿€çƒˆ..." }}
        ]
      }},
      {{
        "dimension": "2. äººæ ¼ç¼ºé™·",
        "summary": "å­˜åœ¨æ˜æ˜¾çš„è‡ªæ‹å€¾å‘ä¸ç¼ºä¹å…±æƒ…ç‰¹å¾ã€‚",
        "sub_items": [
          {{ "term": "è‡ªæ‹å€¾å‘", "analysis": "é¢‘ç¹å¼ºè°ƒä¸ªäººæˆå°±ï¼Œå¿½è§†å›¢é˜Ÿè´¡çŒ®..." }}
        ]
      }}
      ... (ä¾æ¬¡ç±»æ¨ç›´åˆ°ç¬¬9ç‚¹)
    ]
    """

    try:
        response = requests.post(API_URL, json={
            "model": MODEL_NAME,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3, # é™ä½æ¸©åº¦ä»¥ä¿è¯æ ¼å¼å‡†ç¡®
            "response_format": {"type": "json_object"}
        }, headers={"Authorization": f"Bearer {API_KEY}"}, timeout=180) # å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œå› ä¸ºç”Ÿæˆå†…å®¹è¾ƒå¤š
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            content = content.replace('```json', '').replace('```', '').strip()
            
            try:
                raw_json = json.loads(content)
                # å…¼å®¹æ€§å¤„ç†
                if isinstance(raw_json, list): return raw_json
                if isinstance(raw_json, dict):
                    for k, v in raw_json.items():
                        if isinstance(v, list): return v
                return []
            except:
                print(f"âŒ JSON è§£æå¤±è´¥ã€‚")
                return None
        else:
            print(f"âŒ API Error: {response.status_code}")
    except Exception as e:
        print(f"API Exception: {e}")
    
    return None

def update_list_json(region, summary_obj):
    if os.path.exists(LIST_FILE):
        with open(LIST_FILE, 'r', encoding='utf-8') as f:
            list_data = json.load(f)
    else:
        list_data = {}
    
    if region not in list_data:
        list_data[region] = {"region": region, "targets": []}
    
    targets = list_data[region]['targets']
    found = False
    for i, t in enumerate(targets):
        if t['id'] == summary_obj['id']:
            targets[i] = summary_obj
            found = True
            break
    
    if not found: targets.append(summary_obj)
        
    with open(LIST_FILE, 'w', encoding='utf-8') as f:
        json.dump(list_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… ç´¢å¼• list.json å·²æ›´æ–°")

def main():
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œå•ç›®æ ‡æ·±åº¦åˆ†æ (9ç»´æŠ¥å‘Šç‰ˆ) | ç›®æ ‡: {TARGET_NAME}")
    
    if not os.path.exists(DETECT_DB_DIR): os.makedirs(DETECT_DB_DIR)
    if not os.path.exists(DETAILS_DIR): os.makedirs(DETAILS_DIR)

    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        targets_config = json.load(f)
    
    target_config = next((item for item in targets_config if item["name"] == TARGET_NAME), None)
    
    if not target_config:
        print(f"âŒ æœªæ‰¾åˆ° '{TARGET_NAME}' é…ç½®ã€‚")
        return

    filename = target_config.get('filename')
    region = target_config.get('region')
    category = target_config.get('category')
    file_path = os.path.join(PROFILE_DIR, filename)

    if not os.path.exists(file_path):
        print(f"âŒ æ‰¾ä¸åˆ°æºæ–‡ä»¶: {file_path}")
        return

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            tweets = json.load(f)
            if not isinstance(tweets, list): tweets = [tweets]
    except Exception as e:
        print(f"âŒ è¯»å–æº JSON å¤±è´¥: {e}")
        return

    print(f"ğŸ”„ [æ·±åº¦åˆ†æ] æ­£åœ¨ç ”åˆ¤: {TARGET_NAME} ...")
    
    report_data = generate_deep_report(TARGET_NAME, tweets)
    
    if report_data:
        daily_cnt = calculate_stats(tweets)

        # æ•´ç†æ¨æ–‡ (æœ€æ–°çš„100æ¡)
        clean_tweets = []
        sorted_all_tweets = sorted(tweets, key=lambda x: x.get('created_at', ''), reverse=True)
        top_100_tweets = sorted_all_tweets[:100]
        
        for t in top_100_tweets:
            clean_tweets.append({
                "created_at": t.get('created_at'),
                "text": t.get('full_text'),
                "metrics": {
                    "reply": t.get('reply_count', 0),
                    "retweet": t.get('retweet_count', 0),
                    "like": t.get('favorite_count', 0)
                }
            })

        final_detail_data = {
            "id": filename,
            "_fingerprint": get_file_fingerprint(file_path),
            "name": TARGET_NAME,
            "username": tweets[0].get('username', 'unknown'),
            "category": category,
            "daily_count": daily_cnt,
            "analysis_report": report_data, # 9ç‚¹åˆ†ææ•°æ®
            "all_tweets": clean_tweets
        }
        
        detail_out_path = os.path.join(DETAILS_DIR, filename)
        with open(detail_out_path, 'w', encoding='utf-8') as f:
            json.dump(final_detail_data, f, ensure_ascii=False, indent=2)
        print(f"   âœ… è¯¦æƒ…æ–‡ä»¶ç”Ÿæˆå®Œæ¯•")

        # æ›´æ–°ç´¢å¼• (preview å–ç¬¬ä¸€ä¸ªç»´åº¦çš„ summary)
        preview_text = "æš‚æ— æ‘˜è¦"
        if len(report_data) > 0 and 'summary' in report_data[0]:
            preview_text = report_data[0]['summary']

        summary_obj = {
            "id": filename,
            "name": TARGET_NAME,
            "username": final_detail_data['username'],
            "category": category,
            "daily_count": daily_cnt,
            "preview": preview_text
        }
        update_list_json(region, summary_obj)
        
    else:
        print("âŒ LLM åˆ†æå¤±è´¥ã€‚")

if __name__ == "__main__":
    main()